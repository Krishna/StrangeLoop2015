   Camille Fournier:  "Hopelessness and Confidence in Distributed Systems Design"
   
   [Live captioning by Norma Miller @whitecoatcapxg]
     Alex:  Good morning, everyone! How's it going? Ready for another day of Strangeloop?
   All right, good. We do have a few schedule changes today. As you may have noticed, our keynote speaker was not able to be here, so our first speaker here was very kind to step up. And in her spot, her original spot on the program, I pulled in Phil Blodner's talk that he was going to do on an unsession tonight. There is one speaker, Ian Dunning, who is going to talk about Julia optimizations at the end of the day. Unfortunately he was really ill yesterday and decided not to come out, and so hopefully he will feel better, but that talk will be canceled.  If anybody has some amazing thing to fill in for that, ping me and maybe we'll swap something in. It was kind of a last-minute thing. If there's any speakers here that didn't get the e-mail last night, please stop by the registration table for a little gift, we will be happy to get that to you. I don't think I have too many announcements today. There's not anything too different going on from yesterday, so hopefully everybody had a good time yesterday and it will be another fantastic day here today. I would like to introduce our keynote speaker. 
   Camille Fournier, she's the CTO for Rent the Runway, and I'm very pleased we have such a great set of speakers we're able to find speakers as excellent as her in a pinch to do a keynote for us, so Camille.
   [applause]
   >> All right, good morning. Here we go. Very nice. Very nice. All right. Oh, I'm very loud, so hopefully I won't shout too loudly and echo to all of us. As he said, my name is Camille Fournier. I actually just left Rent The Runway after a beautiful four years there, I am happily taking a little break and will be thinking about my next thing, but not yet, first I have to get through this talk. I've renamed myself senior thinker and raconteur. That's actually a Steely Dan reference, I'll let you figure out how.
   This talk is about hopefulness and confidence in resilient systems design. The easiest way to scale, hassle free scaling, easiest database ever, swing state in a cinch at a distributed system scale, real time data, real-time Analytics, real-time decisions. Hope, hope, this is all marketing copy from various different distributed data stores, distributed systems websites, and these systems want you to believe that they can solve your hard problems. That they can solve your hard problems around scaling, because scaling is hard. They can solve your hard problems around synchronization, they can solve your hard problems around data. They can help you make decisions. And then this guy comes along. This is a slide from Kyle Kingsbury's Jepson series. Kyle is a good friend of mine so hopefully he won't mind that I stole this slide from him. And you know, this slide basically encapsulates what he does, right, which is that he shows us how we have these lovely worlds where we have these rainbows of beautiful APIs and some scaffolding and then a tire fire of distributed systems supporting all of that mess, right? And it's scary. And it makes us find of afraid, frankly, I mean Kyle has broken down basically every single distributed system out there. We're hoping he's just going to go in a complete circle and get them all so we can all feel good that none of them work. So you know, we have all this hope that we're going to find the right solution, that we're just going to be able to use something and not have to worry about it and get our jobs done, and then you know, we get afraid ever time we see there's a new Jepson coming out. And they are really fast but you're going to lose data about 10 percent of the time. Hope and fear.
   Distributed systems, they're ugly, they're hard and they're here to stay. Oh. I know, buddy. That's my son. Isn't he cute? And he looks so sad, right? So sad. He's sitting in the sun, playing in water, and he's a sad boy. That's a little bit like us right now, right? We are in like a magical age of computing, and we're kind of sad, though, we're kind of sad about it.
   So how do we deal with this because we're in this hope and fear cycle. Hope and fear are -- it's very hard to have hope without fear. It's very hard to have fear without hope. If you're really truly hopeless, you have no fear. I had a talk yesterday with a friend and he was talking about basic training and how in basic training what they do is they break you down, and they remove your sense of hope, but they also remove your sense of fear in the process, so that you're willing to like get out from behind a bolder and run towards bullets. Hope and fear are two sides of the same coin, right so I'm not going to take you through basic training and tear you down. But I'm going to talk about how to overcome this. How to get away from this, right? How do we break the cycle of hope and fear? Well, I believe that what we need need to do is just accept the fact that nothing is perfect and that we make a lot of decisions all the time, and what is a decision but a tradeoff, right? Tradeoffs are what we need to fully embrace, fully understand, about our systems, about what we're doing, about what we're building.
   We need to embrace hopelessness, give up this hope and fear cycle, and gain confidence, right? We embrace hopelessness gain confidence by understanding that we're never going to build the perfect thing, there's always going to be tradeoffs. So in this talk I'm going to talk about three systems that I have built and I'm going to go into some details, no not a lot of details. So I've gotten to a point in my career where all of my talks I describe them a little bit as Darma talks because you may know take a really specific lesson away, like oh, write your code this way and it will be great.  Use this language feature and hopefully everything will be faster. Hopefully what you will take away, though, is it a lesson for a way to think about the world, a way to be kinder to yourself about decisions that you make, because ultimately they're all a little bit hopeless. But before I get started, we're going to have a brief interlude. The goals of distributed systems, the fundamental tradeoff, right? What are they? Well, I would argue that they're scale. Right? We've got lots of data, we've got lots of people, we want to do lots of things. So we want to scale. And failure tolerance. We don't want to be woken up at 2:00 in the morning. We're in a 24/7 global world so we want to scale but we don't want to fail too much or at all.
   This is the eternal struggle, right? This is the constant conflict because those two goals are fundamentally in conflict with one another. Scaling makes failure tolerance harder. Scaling makes failure inevitable, much more frequently, right? So what do I mean when I say that, right? So when you scale, you have more moving parts beings so there's more independent failure tension potentials, right? The hard drives failed so frequently that they didn't want to bother unscrewing them to replace them. They just wanted to rip them out and stick another one in? Velcrod hard drives. When you have however many hard drives Google has, they are failing all of the time, there's probably three that have failed in the last minute, right? They're constantly failing. So when you start to scale a lot, you have lots of moving pieces that will inevitably fail because mechanical things fail, so you have a lot of pent failures, right, but you have a lot of interdependent failures, right? If that code has a bug, it causes you problems, we hopefully all Dynamo post-mortem. A slight design problem, was manifested by an independent network failure that then caused a cascading effect of failures and a bunch of other systems that depended DynamoDB, right? Scaling up to do more coordination, more communication, right? You have more processing over head, more replication, so these two things are fundamentally in conflict with each other and they are tradeoffs that we are always making, as we're building our distributed systems. How much do we want to push on one end or the other and what tricks do we want to put in place in order to balance our scaling failure tolerance?
   OK. So the first system I'm going to talk about very briefly is a distributed cache for risk analysis. What do I mean? So this system started off as your classic old school 3-tier architecture. The initial developers had the insight of all of the data that we needed to do risk calculations for a very large bank could actually be fit into memory of a commodity server. And that was awesome, because if let the analysts do what I call real-time data mining. It wasn't actually like a SQL prompt but had lots and lots of ways to slice dice the data. Super useful.
   So we knew that we were going to run out of the ability to run this all on a single commodity server and we needed to scale. And that was all we cared about, right? In the tradeoff of scaling or failure tolerance, the only thing we cared about was scaling. We just wanted this thing to keep working, because otherwise, people would not be able to do their jobs.
   Fortunately, we knew about this enough ahead of time that we had some time to experiment with some alternative solutions. So the first time we tried was distributed shared memory, which never works. And this is a really ugly slide taken from a diagram on Wikipedia, but I kind of feel like it's appropriate when you're talking about distributed shared memory to put something that's just kind of like, oh, God, no. Just no. Why didn't it work? OK, if we had been using this as a cache in the sense of like we wanted all the data in Ram so it would be fast but we were only grabbing little bits of it first it might have worked. But we needed to have access to all of that data at once for certain calculations, we were actually doing calculations that would grab all of that data and do stuff to it. So we tried it, we tried some vender product, in java shop it's pretty easy to put your boot loader in and blah, blah, blah and it was a massive failure.
   Then we got to work with one of these things. So you'll recall I said I got to work with gill way back in the day because we bought one of the early Azul appliances. It had a lot of cores, but these cores were slow. And so the idea with these Azuls, even though the cores were slow you could charge your work across lots and lots of course and get equivalence to 16AMT cores that were much faster. You also had the in particular, you could get machines from Azuls that had enough memory to do what we needed to to. so this was after great exercise. We made this work. We didn't keep it. We knew we were never going to keep this around forever, because frankly depending on specialty vendors for key parts of your processing is a risky idea. But we learned a lot about our system in doing this. We learned what we needed to learn to do a specialty distributed system. We learned about data locality, we learned about how to think about the data that we were processing and hard it in a way that we could actually process it in parallel. We could send it to other machines to be processed. We all imagine that parallelism is going to be like this, you set out the bowls and the puppies come and they process their bowls and it's beautiful and it looks like the lower picture, right? This is some Reddit picture or thing. The puppies get food all over the ground, they share each other's bowls, it's a total mess, it's a total chaos. You've got to think about data locality. You got to think about mama dog. Mama dog knows what she's doing here, right? Just the right access points to the data so it can be processed very easily in parallel, OK? Data locality. So by doing all that parallelism, by really having to think about all of the opportunities we had to slice and dice this data, we were able to figure out, oh, yeah, you can shard it this way and you can compute and reduce on the shards. So that's what we did, we figured out a way to shard it. Caitie gave a talk yet on stateful systems, right? It was a lot like that only less generalized. It was a distributed cache of data, where there's shards of data across the machines and a coordinator node sends out a request, each calculates its bit. Great, so we made a tradeoff, though, we did not care that much about failure tolerance at all, we did not care that much about performance, we made a fully synchronous system. The nodes operated in lockstep. We made a lot of tradeoffs in this system, because all we cared about was getting it to work, getting it to scale, getting it to solve the problem that we had at hand, right synchronous systems are less performant. There's a lot more to wait for when you have to wait for every single node to finish its calculation before you show it to anybody. It's also super not failure tolerant. Despite all of that there was a ton of distributed systems complexity that we hit, we used a J group, so that was a thing. But we made the easiest possible choices. We were willing to trade off let's make it synchronous, let's make it a little less performant, we don't care.
   Another tradeoff you might make, they're the tradeoffs that you make when you're living in a world where you're actually doing it, a lot of people wanted to do a big rewrite. They said, you know what, we can rewrite this in a new language, we can rewrite it against a columnar database. It won't be java, they really hated Java, and it will be great. Good luck. Tried that. They did not succeed. Because you know what, rewriting successful systems is one of the hard he was things you'll ever do in your life if you with a happen to do it which you probably will do in your life. It is incredibly hard to rewrite something that is successfully used in production particularly if it's still being developed. The system at this time had a couple of lines of code, had 40 to 60 developers working on it at once. Actually turned it into a distributed system in less than a year. We were able to do that. How are we able to do that? That seems like a tall order. We were able to do that because this system was very well tested. So I know there was the ideology talk yesterday which I really enjoyed. And I think the take away that I will take from that talk to bring to bear here, if you have a system that is not super-strictly typed, for the love of God, you need tests. If you have a system with a lot of developers working on it at once, tests are really useful. This system was exhaustively tested. It was actually originally developed by a couple of -- a team of like XP experts, extreme programming which is one of those agile variants, back in the days when people were not really talking about agile. Tests written by developers and run every time you checked in code and all of that. What that meant is that we were able to turn this into a distributed system and make sure that we didn't break any of the logic by just creating a test fixture that simulated running it as a distributed system. And of course running all the code. Because by the way, this thing wept into production in both single and distributed nodes depending on the data it was loading. We didn't have to worry about breaking all of the logic for every single feature that was being built. Testing meant that we could do this all while the system was continuously being built by other people and put into production constantly. We distributed this system with five people in less than a year while 40 other people were off making changes to it at the same time in single-node mode. Testing. Testing is a good thing. Learn how to do it.
   OK. System No. 2, global service discovery I, using Zookeeper. Oh, look at that. My timing is perfect. This is beautiful. OK.
   Global service discovery using Zookeeper, so I'm not going to have time to plain Zookeeper to you if you don't understand Zookeeper I have other talks on it, you can look at them. Basically what we were trying to do is build a global service discovery system. That would support data centers around the globe, services around the globe, to do service discovery, I know there was a talk from someone at Netflix yesterday about service discovery doing it a very different way than this, and we knew that we had to do it blob globally because we had services that were running in services everywhere and we were trying to make a service for the whole company to use. We were using Zookeeper because frankly there were very few good alternatives at the time.
   We cared, actually ironically mostly about failure tolerance, I don't actually mean losing data. We cared about failure tollance in that we did not want the loss of a data center to take down a region and we did not want the loss of a region to take down all of the other regions. We tired about failure tolerance. We cared about scaling insofar as it needed to handle as many as we could imagine, but this is a failure that is reasonable to shivered, right? You can think of a lot of dimensions to shard this problem if you had to. The naive approach would have been to just create a Zookeeper that had nodes in all the different data centers, that talked to each other and acted as a central coordinator, all the data was written there, it would be super simple, right? You wouldn't have to worry about knowing where to go and whether, you knows, a person over here would see the data written by a person over there, it would all be in the same system. But of course you then get into the most off just tradeoff which is system versus performance, right? The speed of light it's a thing, right? Bandwidth and latency are quite limited. The fallacies of distributed computing, there was a great talk last year on these, if you are unfamiliar with them, just go watch that talk because it's really entertaining and it will teach you them very, very well and particularly the ones I'm highlighting here. Those are very, very much untrue when you talk about the WAN. The WAN, the wide area network going between regions of the globe. You've got sharks eating those cables. Sharks are attacking the typer optic cables because they look like delicious food because they've got repeaters in there, you know, sharks look for electric signals and that's how they hunt which is really cool. But they look like delicious food so that's why they attack them. You know, wide area networks are complex. I did not want to be wasting the precious resource of bandwidth across a WAN which could have been used and needed for things like, I don't know, making bajillion dollars a year doing trading activity or something. Didn't want to have activity going across the WAN to lead to this problem. But that leads to the second tradeoff. I chose a picture of a subway, for those of you not lucky enough to live in that city is that you know, the subway doesn't go everywhere. It will not take exactly where you want it to go, but assuming it's running well, which is, you know, 85% of the time, if you want to get somewhere, it can guarantee you to get there. It's going to have the optimal case experience for you. So we're trading off features versus control. Not as many features, doesn't take you as many places but a lot of control and get you from here to there and really efficiently. OK, so what does this mean for us? So we were not actually building a service, we did not build another system that talked to Zookeeper itself and ran and had state of its own. All we were building in fact was actually a client, because Zookeeper has a thick client that it needs to run for various things. And that meant that we could actually decide what we wanted to put in that client. So we decided instead of having a global system, we were going to build a client that understood that for certain activities , you were to go to the local Zookeeper and for certain activities you were going to look across a series of regional Zookeepers but that we were not going to have one single Zookeeper. Let me try to explain this again in a different way.
   So what does service discovery need, right? You need to be able to advertise your availability and in Zookeeper that means you create a persistent connection to Zookeeper because that is kept alive via system heartbeats. Precision connection to Zookeeper to advertise your availability. If that gets cut, Zookeeper automatically cleans it up. So you want to advertise. But then you also want to look up, right and really what you want to do is advertise to your local region. You don't care about advertising to the globe. You want to advertise to your local region. Hey, New York I'm available for New York service. I'm available for everyone but I'm in New York. When you look it up, you may want to look in your local region. You may want to look up globally if no one is available in your local region. So you can do reads across the globe and we can encode that logic in the client to know, oh, writes are only going to go to the local one, reads are going to go across the globe.  Understanding that we could make this tradeoff, that we didn't have to assume that we had to have one system that was perfectly in synch across the globe, we could in fact encode that logic somewhere else and make it look like a global system when in reality it was deployed as a series of regional systems was a grade tradeoff, which brings us to our next best practice, remembering that clients are part of your system. Both for good and for ill. If you're building smart clients, if you're building thick clients, you can do things like say, oh, you only need to know about the local region if you're advertising and you need to know about the globe if you're reading. Unfortunately clients can also cause you problems in this case, again, look at Amazon, right? Whoops, clients were using features that they weren't really testing that well and all of a sudden you get this cascading failures because everybody adopted this feature that you didn't expect them to adopt. And you didn't think about it. Misbehaving clients when you're talking about shared infrastructure can cause everybody failures.
   So remember, when you're building distributed systems, clients are going to be part of that system and thinking about what you can do to use that to your advantage is just as good and important as thinking about how that's going to cause you problems.
   OK. A second brief interlude before we go to the last and final and most hand wavy of the systems.
   Classic old chestnut of a joke there are only two hard things in computer science: Cache and validation, naming things, and off by one errors. You do not have to give me pity laughs for that. We've all heard this so many times. Why am I bringing this up? What does this really mean? What are you saying when you make this joke, right? What you're saying -- when you say cache and validation is hard. You're kind of saying that disparate systems having agreement on the state of the universe is a hard problem to solve. All of distributed systems talks are basically talking about how that's a hard problem to solve. I liken it to solving world peace, you need everyone to agree on the state of the universe to have world peace. It doesn't happen with humans and it's not going to happen with systems.
   If you name all your variables X, Y, Zfoobarbos. Humans, humans, humans, that brings us to micro-services. Micro-services this is the last distributed system I architected and put into production. I know M word but it's a thing we do these things. micro-services.
   And monoliths, migrating off of a monolith, making it scalable, making it usable when you've got people already sort of using your product and succeeding, is really one of the harder problems that you may ever have to solve in your career, and so you solve it by pulling pieces out into services, that's just a really easy way to solve that problem and it actually makes sense, because the goals, the things that they promise you when you do micro-services, the sort of beautiful dream of micro-service, is that you can scale your systems, and they'll be failure tolerant, because you can have independent sis deployed independently and if this system fails, this system is not going to fail, certainly.
   But you can say human, go fix that, put this feature prod, if it works, great.
   >> OK. So what are some of the tradeoffs that you make when you make this decision to go to micro-services and especially if you take it to the far extremes, right? The tradeoffs of micro-services are just as much human tradeoffs as they are systems tradeoffs, right? You trade off common understanding of the state of the world. When you are all working in the same code base, when you are all working in the same language, when you are all talking to the same database, you have a shared understanding of the state of the world on your team. If you decide to go all the way to the far extreme of micro-services that is the beautiful dream of, you can use whatever language you want, you can use whatever stacking store you want, you can use the perfect tool for every job. Over here it's Java, over here it's C++  , over here it's Clojure, whatever, who cares, right? You take that common understanding and slam it on the floor and it breaks it into a million pieces. Perfect tool for the job can be valuable sometimes. Sometimes you literally have a problem that cannot be solved in maybe your existing infrastructure and you have to do something different and it's good to have the ability to be flexible but if you go to the extreme of the perfect tool for the job, you're sharding your humans, like one or two people per tool, and here's the thing about failure. Right? Independent failure,ing goo will is ripping out hard drives? People quit. People quitting is like hard drives failing at Google. It happens all the time. You can be the best boss ever, and it will happen. You can be the worst boss ever, very common, it will happen. People move across the country, people decide to become artists, people just get sick of your shit, whatever, people will quit. You have to expect that people will quit and now you have much less failure tolerance for your humans. When people quit and they were the only people that understood a system, wow, you've got a big failure risk running in production right now.  This sort of takes us to a similar related tradeoff, right? Rate of change, thoroughness, or efficiency thoroughness, the ETTO principle. I came across this when I was writing this talk. You want to be scaling, you want to be efficient in all the features that you're getting out the door, right? The efficient thoroughness tradeoff, demands for efficiency, AKA scaling, AKA getting more features out, whatever, reduce safety error tolerance, whatever, right, demands for safety or failure tolerance reduce efficiency. Basically the scaling makes failure tolerance harder, failure tolerance makes scaling harder, right? When you are trying to get a bunch of things into production, you're going to have more production failures, you're going to have more errors, you're going to have more bugs. I promise you. This is going to happen. This beautiful dream of micro-services is, you know, if you were really doing it because you want to scale feature development in particular you're probably going to have more failures, right? You can try to Gat this with monitoring and automation, that's the promise. We just put some monitoring and automation on you're going to be fine and that's great. But those things require up-front cost and they're going to temporarily lower efficiency and frankly they require maintenance, you have to constantly be, working on your monitoring, constantly, working on your automation, so you are trading something off if you want to scale a lot. You're going to have more bugs, you can slow down that scaling a little bit to put in things to support, you know, making bugs less likely, making things less  failure tolerant. But that's going to be a tradeoff, you cannot pretend that it's a perfect world, all of a sudden you can just blast features out as fast as humanly possible. It is always a tradeoff.
   So I took this from Adrian Cockcroft who gives a lot of talks on services, and I found this slide to be terrifying. I think he also says it's terrifying. I think it's terrifying. What's happened, right? You've moved to micro-services, you used to have one big centralized complex system with fairly simple overall system, right? It had some ex connections to databases and whatever. But now you've got all these little external points talking to each other and the external points hopefully themselves are very simple but you've got a lot of complexity still, you can't kill complexity completely, you can't remove it from the system entirely,s there he a going to be some complexity, you've now moved it into the interconnections and interdependence of the services that you've built.
   So how do we get around this? How do we deal with this challenge of, OK, well, we knew we needed to go to micro-services, this is ha necessary evolution for us, but now we've got this type of complexity, what can we do? Because remember, all of this interdependence can cause you scaling problems in reality, right? Data design, data design, again, thinking about your data. Thinking about what your code is actually doing is the only way to solve this. OK, what do I mean when I say that?
   Well, we had a speaker yesterday talk about this. Little finger. You guys know who I mean. Oh, no, no, Peter Bailis, not Peter bailish. This guy, this guy, Peter Bailis, he talked about this yesterday and he gave a keynote at MESO conf a few weeks ago that I really recommend where he talked about coordination systems silence is golden, right? If you have to talk, if you have to communicate, if you have to coordinate, if you have interdependence, scaling is hard, and failure is going to happen more and you're going to have to do more work to avoid failure. Thinking about your systems, thinking about where your data actually needs to coordinate, and where it can be totally independent, is a key design skill for building successful scalable micro-services as much as it is for doing anything else.
   Silence, silence is golden? We don't have right now that the systems are still in research to let you hint to them about coordination at the data layer and research is interesting, maybe in 20 years it will become reality, right? That's sort of the time frame. Maybe less than 20 years these days, but you know right now we've got the no SQL stores and the SQL stores that we have and the file systems that we have so we've got to be a little clever but you've still got the ability even today to build silence into your systems, to build systems that don't have massive interdependence, right? You can always build a big ball of mud, you can always build a system where you've where you've got these cyclical interdependence that's a frickin' Nate mare, right? You are not going to be failure tolerant if you build your systems white thinking up front about your data design, about what can truly be independent and what in fact has to have a little bit of interdependence and where you can sort of tweak it. A lot of times in the reality in in the products that we're building if we're building systems for real-life applications it's OK if you have a cache and it's a little stale, right? It's OK sometimes even if you misan update to a cache occasionally, eh, whatever, your customer is going to be a little mad if you charge her credit card multiple times but she will accidentally forgive you if she sees an old image of a dress and she sees a new image of it, right? I think one of the biggest points I would like to make here also is human fundamentally understand asynchronicity. They understand that if they put something in their shopping cart yesterday and they don't check out and they come back two days later, that thing may not be available. They understand that the state of the world is always changing and they can kind of deal with it. People are smarter than we give them credit for. Right? People who are not tech people are smarter than we give them credit for, right? They understand the nature of the world is that there are not as many guarantees in a lot of the things that they do, and they're forgiving of certain kinds of errors, they're very unforgiving of errors related to money but they are generally forgiving of a lot of other errors, so think about that as you're building your systems, right? You don't always have to be exactly exactly perfect. In fact, perfect correctness may not be the goal. I think worst-case design, designing for the would worst case, that was Peter's message yesterday, that was great. Identifying what your worst cases might be is a great thing to do and then identify which cases you just don't care about because sometimes you are just don't care. You know what my business if I lose the east coast of the United States, oh, well. That's not true for everyone but that can be true for you. That's a worst case that you can accept at certain stages of your business. You have the ability to make these tradeoffs and to make these choices up and down the stack and that can sometimes include the features you show your customers, right? But it has to start from the top. You have to think about your data design. If you care about silence, if you care about actually building scalable, large-scale application systems with lots and lots of different moving parts.
   OK. So I think I said earlier that this is a bit of a Darma talk. What does that mean? A Darma talk is -- you may have thought of so many failures in the systems that I designed. And you may be like oh, my God, why is this woman up here talking to me? I can think of lots of failures in the systems, too, in fact the distributed cache, I think a couple of years later, the actual application code still running in prod, but it now runs in SSDs. So on that distributed system, eh, no more, whatever, right? The distributed global discovery service, probably if I was doing it today I don't know that I would use Zookeeper for it. Maybe I would, but maybe I wouldn't. The world is constantly changing. The only thing that you can guarantee in this world is change. So you're going to have to do tradeoffs and you're you are going to have to do the best that you can accepting that you will make mistakes and even if you develop something beautiful and perfect it will not last forever because change is inevitable and that is OK.
   Distributed systems are ugly, hard, and here to stay. And instead, come on, no, manatee, oh, I know, it's really tempting to want think, oh, let's turn it all to the ground, right?  We're not going to set the world on fire just because systems are difficult.
   Instead we're going to understand that everything that we do is a tradeoff, that we are making choices and we're going to make wrong choices sometimes and that's OK. We're going be to hopeless, we're going to be confident in your hopelessness because we've gotten away from that cycle.
   Everything is fine. We're going to meditate in the flames of the tire fire. We're going to enjoy the flames we're going to warm our hands, are we're going to try to avoid the toxic fumes. It's fine. It really actually truly is fine. You're going to be OK. Be joyful. Enjoyed ride and the last thing I always say to my audience these days is test your frickin' code. Thank you very much. Special thanks to all of these amazing people. Thank you to Alex for having me. This has been a really fun and I am not taking questions right now, but if you want to tweet me or find me in the hallways, I will happily do it then. All right, ... ... :
   >> Thanks everybody we'll be back in all of the rooms starting at 10:10 so we've got a little bit of time, so.
