   Peter Alvaro:  "I See What You Mean"
   
   (Live captioning by White Coat Captioning @whitecoatcapxg).
   >> Alex:  Good morning, everyone, thanks for coming out!
   [applause]
   >> Welcome to Strangeloop 2015. I'm glad you're all here, glad you all made it. Hope you had a good time at the party last night. Big thanks to the party sponsors last night and the bus company. I heard they had some heroic measures at the end of the night, so thank you to them. Thanks for the catering and everybody who helped all the staff who helped make that a success last night. I don't know that Nick is in here, but Nick is the organizer who sort of focuses on the party and so I'd like to say thanks to Nick for all his hard work.
   [applause]
   I wanted to remind everybody that there's a code of conduct for the conference that you can find on the policies page of the conference website. If there is anything you need, there is a duty officer in orange at the registration table, so feel free to -- usually it will be Bridget, so feel free to talk to Bridget. She's awesome. Lunch will be provided here today. There will be a lunchtime talk from 12 to 12:15 by our local launch code program, they have sort of a side group called Coder Girls and so they're going to talk a little bit about what they do. If you're interested in that, feel free to stop by, it will be right here at 12. So you need to bring your lunch in and hang out. I want to thank our platinum sponsors, Cerner and sparks, big things to them. And there are a lot of other sponsors and please check out the signs around the building or the tables, and the ticket grand lobby and the mezzanine, make sure to stop by there or check them out on Guidebook so the information is all over, and if you see somebody from one of those companies, please tell them thanks and they helped make this happen and make it possible, so thanks a lot to them. Guidebook is the mobile app if you don't have it yet, it's got the full schedule on it. You can click on the location in a session, and it will take you to the map and tell you -- show you where that is. There are -- we're in the main theater right now, which is where the keynotes will be. There's a ballroom on that side, a ballroom on that side, and one upstairs and upstairs, and so those are kind of the four places and there are signs and things like that around the building if you need help finding it. I think most people counseled the Slack chatroom so far. Seems like that's going well. It's been a good place. If you do have problems today, if you see temperature issues, WiFi problems, that sort of thing, please drop a note in there and we will try to address it as quickly as we can. And I think that's the majority of the announcements. I'm just really happy to all of you here and it's going to be a great show. I do want to apologize publicly that Kathy Sierra will not be able to make the morning keynote tomorrow. She did a an accident this week and -- I can't express to you how bummed she was that she have had not going be to able to be here. She's been looking to it for many months and we've been talking over a year now for the keynote so that the a bummer. We are very fortunate that we have such a great set of speakers who are here for the rest of the conference and so Camille was very gracious in offering to step up and act in the keynote slot and I'm very thankful to her and looking forward to her talk tomorrow morning.
   >> I should -- I guess I should mention, tonight are the unsessions and those are back at the Union Station. So all the conferences here today and tomorrow, but the unsessions are tonight, back in the Union Station. They are in sort of the train meeting rooms that are -- we had many so of the preconference workshops, so New York Central, Illinois Central, ... so if you want to see those they're available from 7 until 10. And those are the schedule for that is on Guidebook and on the website. I think that's all of the announcements we have. Is everybody happy to be here?
   AUDIENCE MEMBER:  Woo!
   [applause]
   [cheers]
   >> So it's always a blast to track down the keynotes for Strangeloop. It's daunting sometimes and it takes a lot of logistical work, but our first speaker is someone that I asked him, and he's like, oh, yeah, I was going to submit the CFP and I forgot and so you know, whatever.
   [laughter]
   But he was -- he immediately got back to me and said that he was interested in doing it and that's always fantastic when that happens and when the speaker has actually heard of the conference and things like that. So that makes things easier. So it's my great pleasure to introduce Peter Alvaro, who's Assistant Professor of Computer Science at UC Santa Cruz. Thanks.
   [applause]
   ...
   >> Hi, everybody. I'm Peter Alvaro, I am an Assistant Professor of Computer Science at UC Santa Cruz. I'm thrilled to be here. Morning keynote slots make a great time to catch up on sleep, so thank you for not doing that. Thank you for being here. This talk is called, "I See What You Mean: What a Tiny Language Can Teach Us About Gigantic Systems," and it's really the back-story of a tiny little boutique language called Dedalus which sort of formed the rock on which I built all of my thesis work, and hopefully with any luck, another ten years of good research. 
   I never got to go back and tell the story of that little language, the context out of which it arose, some of the design decisions we made, so I want to thank you for being my captive audience in this somewhat historical narrative and suspect like many historical narratives, there's a little bit of meandering in there. So I found it a good template in talks like this to lead in with some very strong statements. You know, statements from my gut that you may not like. Something to get your bile up. Statements by the way of which I have no intention of backing up with evidence, because they just represent my beliefs, you know, but I hope that they'll form sort of a semantic thread that I can hang some of the technical material that's going to come along the way on. Because full disclosure there is a talk that's largely about semantics. So there will be a little bit of technical material along the way. 
   Polemic time: Abstraction is sometimes harmful. So abstraction is a thing. It's arguably the best tool that we have in computer science, and cognition generally for managing complexity and for moving beyond it.
   And at least in computer science it works like this: We labor at something with computers. And we build an object of significant complexity, and perhaps even proceed found beauty and then, you know, we finish laboring on it and it comes time to think about the ways in which it will be used and reused so we draw a line of demarcation over that object and we ask ourselves what are the details that are essential in is well, what does essential mean? 
   Well, essential to some domain that we think the object will be used in. And we obscure the other details of the object, right? We do this any time we take a bunch of code and put it in a function and make decisions about what parameters to expose, what to return, we do this when we group together a bunch of functions with related functionality into a shared library. We do this when we group together a bunch of functionality behind an API that we expose in our service boundary, but nowhere are these sorts of distinctions so stark as when we design languages and then of course the hope is at that interface boundary we expose those details that really matter and then a thousand flowers will bloom, right? You know, thousands of applications, applications we could never have predicted will use our interface and make progress without having to concern themselves with the hard-core details of the implementation, right? 
   And that's a good thing. And sometimes it's even better than that. Sometimes the act of hiding can illuminate surprising things. If you blow away a haystack, all of a sudden there's all these shiny needles, right? So in this talk I hope to show that by papering over a lot of distinctions between state representation and different patterns for control flow and synchronization, oops, that should have gone away, too, we can illuminate the real matter in a domain of distributed systems. Data moving and changing through space and time.
   OK?
    But it's not all good. You know, when we forget and we do forget, that abstraction is a universal activity of computer users, it's something that we all do to manage our own complexities. We get lazy and we tend to think of abstractions as these fixed boundaries, you know, these walls. And walls are intended to create borders, to prevent things from flowing across them.
   Worse still, walls pen people in and when you think about it, who do we put below the wall on the hard side? Well, we put us, right, the geniuses, the infrastructure programmers, the 10X engineers, the library programmers, right? And who goes behind the wall? Well, the despised users, the programmers with neckties, the SQL people, the analysts. 
   We hate those guys, read the fucking manual, we say to those guys, and it's a sign that's always made me really sad this belief that library users and users are two different classes of people. Life or maybe just the arc of an application's life or something like that, it makes me sad. But it also makes me concerned. Because if respect those users, by the way, those users are the people who are using computers to do real shit, to do things done, to do science, they're the reason we exist, right? If we respect them so little what are the odds we're providing the right interfaces for them. One other word about abstraction.
   [laughter]
   There remain certain domains I'm thinking about one in particular that remains sun stubbornly difficult to create the right abstractions for, right? It would be extremely tempting to paper over the complexity of physical distribution. It would simplify the models we use to interact with large scale systems, but unfortunately attempts to do so represent some of the worst failures of abstraction in the history of computing, I'm thinking about stuff like RPC, you know? I'm going to put a function on some other computer and pretend it's on mine. What could go wrong? Or I'm going to put a disk in another computer and prevent it's in mine, what could go wrong? The jury is still out on this, but it does seem kind of dicey that you could take a databases that's literally distributed all over the world and pretend it's in the tower under my desk. We can paper over these things and they're going to do more than leak, right? So OK, enough about abstraction. Let me talk about me for a minute.
   I'm not a big fan of about-me slides, but I can't really tell the story of Dedalus without telling little bit about the story of me. And the first thing to get out of me is I'm not a lifelong hacker. I barely touched computers in college apart from word processers and it wasn't until I moved to the Bay Area in 2000 and I had to get work, that I started writing code for a living. I was working for a company that you may have heard of called Ask Jeeves and my team was called data engineering. We were responsible for the whole life cycle of user data at the site. So I was living in two very different modes at Ask Jeeves. I was writing plot of distributed systems code in a language that you may have heard of called C. And then I was, you know, moving the data around, replicating it, transforming it, loading it in the database and then once it got in the databases, I was writing a lot of -- uh, I was writing a lot of reports on it in a language that you might have heard of called SQL. So it was really had two hats and switching back and forth between them and when I used C I felt like this guy. Because what I was doing was really hard, but the thing was, I was good at it, you know, I felt like a super hero. Man, it feels good to write a big chunk of C code. If you think it feels good writing it, you would love debugging it. But that was my comfort zone, you know? And when it came time to write those reports, I felt like this jerk, you know? Everybody hates SQL, right? The first was if you kind of squint your eyes the work that I was doing in those two modes, it wasn't really that different. I was data wrangling. I was giving rules about how the data was supposed to move and how it was allowed to change and so on. And there was something kind of perverse about the infrastructure programming I was doing in C, solving those same problems over and over again, you know? And I began to ask myself, gosh, oh, the other thing to point out is it felt good to write C. But when I wrote SQL, I wrote good programs. So I started to ask myself, gosh, could it be possible to use a language like SQL to write general purpose systems? What would have to change to make that true? And I could explain these feels, the feeling good thing, when we're writing programs like C we're interacting with computers in the way we were trained to. Like, I learned on Texas Instruments: So when we think about interaction with computers, it's very natural to think about what a program does is it provides us with a mapping from the program to a set of behaviors that it induces in system. That's where we like to be. That's where we're comfortable. A language like SQL by contrast gives us a map of a program and the outcomes it produces. Jumping over the behaviors. It means well, if you will, because data becomes a common language between the program that the programmer is laboring over and the meaning of the program. The place that the programmer's head has to go as soon as they lift they are heads from the keyboard. I'm arguing semantics, right? There's lots of different flavors of semantics, but two that I will point out right now, many of you may be familiar with the operational semantics of programming languages which essentially say a program means what it does to some abstract machine, its behaviors. The model theoretic semantics, by contrast, semantics that are usually in formal logic and to a lesser degree logic programming say that no, no, the meaning of a program is precisely the structures that make the statements in the program true and if our language is a first-order language like SQL, the meaning of a program is the databases that are consistent with it, meaning is just data, a common language between the program and the semantics.
    OK? So like here's a boring old query with a somewhat platonic bent. It says, you know, all humans are mortal, right? In fact, let's just write it down equivalently in first-order logic to make it crystal clear that all humans are mortal. We ask what does this query mean? We've written it this way, it's easy to see that what it means is it's an implication. It has to be consistent with the. So we say what are some structures that could give it meaning? Well here's an structure that assigns individuals to a table called humans and this is not a model of the query, because this is a dangling implication. The premises are true, Socrates is a human, but Socrates isn't immortal. This tells us that if we want to make a structure that is consistent with that, we have to add at least one more table. So the meaning of the query is what the query does when you run it, but it's also a structure that makes the query true, so you know, loaded question, model theory helped us write correct boring business logic in dusty old databases, but could it help in a really stubbornly difficult domain like distributed systems? We would want a language that made it easy to write the good programs, despite the bad things that happen. A program that -- a language that makes it easy to write programs that can tolerate failures, like computers going down, like networks partitioning and that is tolerant to reordering in the network that happens for a variety of reasons, right? And it's hard to reason about whether my C program does the right thing in all these contingencies, simply because there's too many possible behaviors in that space, let me give you a little back of the envelope thing: Let's imagine that two end points, Alice and Bob are trying to communicate over an unreliable network and for those of you who are about to pull TCP out of your ass, just imagine that.  Let's keep the example simple. So Alice wants to send five messages to Bob. She does so using a function G, and Bob when he receives the messages computes a message F. A function over all the data. I read somewhere that that's called the lambda architecture? We have to consider all of the executions, for example, here's an execution in which messages 2 and 3 are dropped and 4 and 5 are per muted. Does it do the right thing in that one? Well, we could scratch our heads and say yeah, F does the right thing in that one. How many times do we have to consider this? If we consider loss, we have to consider the subset of all the subsets of those messages. That's a big number. Even if we couldn't consider loss, we have to consider all the permutations. It's like for every element of the power set we have to consider the factorial and take the summation. it's math. We have to crack into the semantics of these functions F and G. Obviously is F is commutative, we don't care about that factorial figure any more. if G retries often enough to overcome failure, we couldn't care about that power set figure anymore. But, we might be concerned about what F does when a mess handling is delivered more than once, which it surely will be if G retries, right so we have to convince ourselves of another algebraic property of that. And if we convince ourselves of all three of those things, victory, right? How do we convince ourselves that our program has these algebraic properties? Genius maybe? I don't know. We have few tools to assist us. So queries seem promising, but there would appear to be this expressivity. Well, there does seem to be an expressivity issue with queries, right? The data in the query model is just sort of sitting there and queries walk up to the bar and they kind of unroll themselves, right? Into a tree, a data flow tree of operators, relational operators and when they reach the leaves, they pull the data and data flow goes up the tree and that's all well and good. But that doesn't seem like programming, right? That's like Q & A. Of course we have this other model of queries that has been the extreme model. It's very much the dual of the other model, right? What's just there is the queries, they're like instantiated in the network and the data walks up to the bar. And data and control go in the same direction and that's pretty interesting, but it gets really interesting when you start imagining hybrids of these approaches. Like what if I imagined a stream of requests, like API calls, and then I do some pattern matching, I filter them. I probe some static internal state. If the lookup succeeds, I generate a new event which sends a new message. This is starting to look like a very general model of computation, this is kind of akin to event loops in an event oriented language like Erlang. When you start to look at the world in this way you start to have these aha moments like oh, my gosh, a web server is just like a join between a stream of requests and some static content, right? All you have to do is squint a little bit and say I can see a bunch of ATP calls as records, and I can see a bunch of files on my file system as records and it's just a matter of joining them up and projecting them and some of you are probably saying, oh, please, man, this isn't a web server, this is just a high level sketch of what a web server does, this is boring, sure, fine, fair. So Hellerstein's inequality establishes that data independents, the property you get when you separate logical representation from physical implementation is a huge win whenever it's a case that the rate of change of the environment is much greater than the rate of change of the application. And this is a slam dunk in dusty old databases because your like boring business logic like how do I know what floor Scott's on and when he's eligible for a promotion change to physical conditions like if you swap out a disk, you update your hardware, but I would argue that in a cloud world the magnitude of this inequality is so much greater, right? The right decisions about the hard stuff implementing a web server like what data structures and algorithms should I use, what caching strategies should I use? What's a good partitioning that avoids hotspots? The answers to those questions are likely to change all the time, maybe at every request and it would be a huge mistake to delegate responsibility for fixing those choices to some genius programmer, right? It's a kind of decision that you would want to offload and I don't want to trivialize the difficulty of this, about you you want to offload to an optimizer. So queries seem promising, expressivity is. Lets ask what we have, what we need and what we don't need. So as I already showed you, most query languages like SQL are composed from a small selection or filtering, projection or transformation and join or composition, and if we take just those three things together, we get the conjunctive queries. We can ask simple questions, like give them what I know what follows. And SQL actually adds the notion of negation to this algebra, so I can ask questions give me what I know and what I don't know what follows. I could add, if you like, a black list to my web server. Now, unfortunately, though, classic SQL doesn't have any way to express recursion, iteration, looping, features that we usually think of as being fundamental to computation. But there is a language that I encountered when I first joined Berkeley that allows you to define it that's called datalog. I but what the power of datalog, I can add a file system to my web server whose directory path structure is recursively defined as a tree, right? I can say the path to a file is slash and the file if it if the file has parent no parent.
    So that's powerful. But there is this question, what happens to that beautiful data-centric model theoretic implicative semantics if we have looping? Does everything go haywire? Well, luckily it doesn't. The meaning of a program is its minimal model. Model I've already sort of showed you. There's nothing missing, there's no dangling premises. Minimal means there's no extra crap. There's no unsupported data that isn't -- that wasn't derived by one of the rules. Let me show you precisely what I mean. So here's a really simple datalog program. By the way, datalog programs can be read simply with the premise on the right and the conclusion on the right. The way to read it is if the left-hand side is true with some bindings, the right-hand side must be true with some bindings. So when he we ask ourselves what this program means, we provide databases to the program and ask if they're minimal models and so like -- we can throw away this one database right away because it's not a minimal model. It makes the premise true, but not the conclusion true. And we can throw away this database, too, because although it's a model, it's not minimal. It has extra crap in it. It says we delivered bar when it was never pending be R, we take that away it had it leaves us with the meaning of this program. Exactly this database. So the programmer writes the program, they're imagining that database and that database is also the semantics of the program, right? Two interesting things pop out. One I've already said data becomes the common language between the programmer modeling the program and the world of meanings where they go to ask if their program is correct. The other interesting thing about datalog is those outcomes are completely independent of execution order. It doesn't matter what order our data appears, it doesn't matter what order we schedule computation, there's one meaning of the program. Here's a very famous datalog program that computes a transitive closure of a link graph. It says if there's a link from 1 to 2 and if there's a link from 1 to 3, there must be a link from 3, right? Our demonic scheduler can just randomly pick fire them, draw conclusion, jump around, doesn't matter what order this happens in, we produce one unique answer, the fully connected paths. So many behaviors, one meaning.
   We've got this mapping that we wanted. Now, when we write programs, a lot of the time we could reason between programs and outcomes but sometimes we have to reason in the reverse direction, like when we debug, there's some answer that we got from our program rung, we don't understand why. We want to know why it produced that. And that would appear on the surface to require us to peel back that veil and look at the ugly world of behaviors of L. But luckily when we use datalog that's not true. So here I have an answer there's a path from 2 to 4. That I might be curious about why it's in the answer and if we collect lineage, we get a very correct data independent answer. You had this data in your model and it was implied by this rule given that data and then transitively down the tree and that data was implied by other data given other rules and so on and so forth. This gives an order independent explanation of why something was in our output, which is nice, but even more interesting than that, it allows us to ask and answer into what if questions. For example, if we could ask well, what if I deleted some of that P. base data? Would I have to delete the drive data it's and easy to see that if I deleted that data I would lose one of my proofs but I would still have another proof. Now, this kind of reaping in dusty old databases for something like maintenance. But it gets really compelling when we think about doing this for a distributed system and representing some of the inputs as facts like, this replica is up, or this is not partitioned. Because then we could ask questions like would the outcome still be goods if I lost the replica? Spoiler alert.
    So datalog is good. A really nice property to have in environments in which we have limited control over the order in which data appears and the order in which computation is scheduled environments in question controlling those orders may have prohibitive costs K.
   It's tempting to ask, well, what happens when we put negation back into Dedalus? What happens to our semantics then?
    And I'm sorry to say that everything goes to hell then.
   [laughter]
   So like here's a program that I don't recommend you write. That uses self-reference and negation. It says you have some input, you ever some action. If you haven't aborted, you can commit and. But unsurprisingly this program doesn't have a unique meaning. It has two alternative meanings. OK?
    We could commit or we could abort. Well, that's not that interesting, because that's exactly what the program says to do, about you what's interesting is which meaning we end up at is going to depend on the order in which we schedule the arrival of data and the firing of rules, right? So for example if I light up this data, and then I light up some real arbitrarily say the first one, I substitute it in and I haven't concluded abort, so I can conclude commit, I put commit in my answer and I say yes! Commit! And then maybe in another replica seeing the same order. The other replica concludes abort and that would suck. Conclusion is when a program randomly as an -- oh, yeah, the point to make is when we go to fire that other rule that would lock it. But it would if we did it in the other direction and we shouldn't be that surprised that when we arm ourselves with the power of self-reference and negation, we can get ourselves into trouble, right? We can say crazy shit, because with the power of self-reference and negation, we can start to utter paradox. The basher is the man who shaves, all the man who don't shave themselves is this sentence is true or false. I can't think of any logical paradox that don't require self-reference and negation exercise for the viewer. So I like query languages because when you write a O. query, you mean what you say. Datalog is interesting because we see that there's this rich intimate connection between talking about what you don't know and having to commit to particular orders to get deterministic results. Datalog is also interesting because lineage is cool. Lineage reveals redundancy and independence of support for good outcomes. We can use that to reason about whether those outcomes are robust to bad things that could happen in an execution.
   But we're still not quite there, right? This kind of like the work that I started to do once I was at Berkeley and got frustrated with datalog because it didn't do everything I wanted it to do.
   To turn this event loop gadget into a protocol at the very least I would need to instantiate more than one of them, right? And then I would need to arrange them in such a way that you know, the conclusions of one became the premises of another by sending messages, right and in fact it might even be cyclical. But I have to admit that those dotted lines are different from the other lines, right? There's uncertainty in those dotted lines because I'm communicating over an unreliable medium, I don't know when those messages are going to be received. I don't know if they're going to be received. Something unpredictable happens there and it's worse than I make it sound, because in general, protocols don't just have some static state that dictates their behavior based on how they react to messages, they have dynamic state which changes as a result of the messages they get. Messages change their state, that changes their behavior and the messages that they send, we have to worry about something unthinkable, unthinkable happening in this loop, right? We have this back-loop in which we draw conclusions from nondeterministically ordered events and that changes our state, right? So if we're going to write a language that really captures everything that happens in distributed systems, we're going to need to capture time-varying state, that horrible thing that we all hate so much, right, mutable state and we're going to need to capture nondeterminism in the execution, reordering of messages, loss of messages, loss of nodes, and so forth and you know, if we could nail these two things in a nice way we would have really cracked a nut on everything that's so hard about distributed systems, right? Because distributed systems are hard precisely because disorder happens and then sometimes we witness the disorder and it changes our behavior, disorderly environments become disorderly behaviors. So how should we even talk about state in a query language? It kind of almost seems, I don't know, like a contradiction in terms, right? Well, we shouldn't talk about it like this, right because we're trying to be logically oriented and this doesn't make any sense. We're talking about some value in some time and being the same as some other Sam. The only way I can get my head around an assignment statement is well it partitions time into the past and the future, right? Which means you know there's a clock hidden in there. Every time we pass an assignment statement we move from the past to the future. When there aren't assignment statements we necessarily have to do that. And similarly when we communicate over an unreliable network, the send happens at some time, let's just call it the past and the receive happens, I don't think we can say anything about when the receive happens, right?
   So the whole idea behind Dedalus was you guys around talking about time, you need to talk about time. If query languages talked about time, we could capture all these patterns, be honest about what we can't control, be powerful will what we can control. so the idea is we want to take that clock and reify it. Make that clock the unit of every piece of knowledge that we have: Can we talk about time for a second?
   You guys are all familiar with the conventional model of time in which we live our lives in time marches inexorably forward.  and the database model of time is actually very similar, because in databases, although there's concurrency, we have a concurrency mechanisms that linearize it, right? Except, when query languages walk up to the bar, they can't talk about time. They query an eternal present and all the dots sort of converge to one. You're probably also familiar with the distributed systems model of time, in which processes have single timelines that make sense and they march forward in time but when time comes to send a message, that message moves forward through space and time and we can say forward in a well defined ways, because we can posit the existence of a sort of idealized timeline that none of the processes can view that moves forward and that's what keeps those lines going forward. But if we want to write a language in which time is explicit. We certainly can't allow that language to talk about the God Line. So forget about the God Line, and when you forget about the God Line, you have to admit the possibility that messages can go backwards in time, at least in the shallow sense that the time stamp on the receiver when he gets the message is lower than the time stamp on the sender when he sent it. You'd have to write protocol in Dedalus. Now, it might be tempting to rule out causally impossible executions, like this one. I send a message to some other person and she replies and I get the reply before I sent the message. Because that just can't happen, right? That's crazy. Well, I don't know, like what if -- what if we did that execution, we sent the message and then she went into like a long GC pause and I crashed and I started to replay from log, OK? So like -- it's not clear to me that that can't happen. So I think we can't constrain receipt time whenever we communicate over an unreliable network at all, not in our model, not if we want to be honest about everything that can go wrong. So you take all of that and you come to like the Dedalus philosophy. So the first big shift is knowledge is not a persistent thing. Knowledge is just ephemeral. Here's some knowledge, in a string called data. In a table called data, but it only has meaning at some discrete time, 5, on clock, knowledge is ephemeral, knowledge is local, knowledge is a flash in the pan, OK? And viewed in that way, the better part of computation is just setting up rendezvous of ephemera. Right? Like I can't conclude something from something that I know and something that you know. We have to, like, talk, right? And moreover, they have to be true at the same time, at the same place. I can't draw a conclusion from something I know and something that I don't know yet any more than I can draw a conclusion from something that I know and something that I've forgotten, so computation is rendezvous, OK? When you look at how rules look in Dedalus, they extend datalog with three types of temporal rules. The first one so-called deductive rule is just like datalog. Given what I know, what else do I know, now, now, now, now.
   The second type of rule is an inductive rule: Given what I know now what is true at the immediate next time? The clock moves is the point. And this allows us to talk about relations between successive states in time. And the last type of rule are asynchronous rules. They allow us to be honest about the control that we don't have on the clock when we recommunicate over an unreliable medium.
   The premises of asynchronous are true at some time, the conclusions are at a time God knows when.
   Every record has a time stamp, deduction rules state that the conclusion has the same time as the premise, inductive rule and asynchronous rules say there's this infinity loop of time we randomly picked from it, sorry.
   We don't have to talk about the time stamp, we don't have to talk about the constraints, we can just put modifiers on our rules, same meanings. Now, next in addition to expressing relationships among facts, it allows us to characterize relationships between states in time and so I can say these two things have to occur together or not at all. Or mutual exclusion. This thing happens and then that thing happens, sequencing. Three features that we couldn't express in a query language that now allow us to do things like implement concurrency protocols.
   Another mindbender is that unlike in databases which having no time, had only state, in Dedalus, theres no state. State is really just a programmatic construct. State is it what you get when you know something, then you know it at the next time and by induction you keep knowing it, right? So this set is immutable because if I put it it will be truth the next time. State is induction in time. OK? Like the way that frame worlds work in logic or the way that DRAM works in your computer.
   Viewed this way, state is not our enemy. State's our dear friend. State is what makes the otherwise unlikely rendezvous of ephemera possible. If I know about two events I should dispatch a new event, what is the likelihood in an asynchronous event in an environment will co-occur. Well, state is what will make them co-occur, OK? State is our friend but is a loose cannon, right, that we need to keep on a tight leash. Here's a register, a fundamentally order-sensitive, imperative pattern written in a query language. We say, if you get an update, you put it in your register. Oh, and if you don't have an update, you persist the current value of the register. So obviously this is a gizmo that's going to be sensitive to the order in which its updates occur and for which the last writer withins.
   And here's its brother, first writer wins.
   This is an interesting gizmo. And we can generalize a register to a table where we have to explicitly delete and we can accumulate data and it all makes sense as we go through time. Mutable state in a query language or the ubiquitous fragment of the ubiquitous query counter problem, right? If I have an event, I should bump my counter and if I don't have an event, I should preserve my counter, right? So next was countable. We need to come clean about when we can't control the hand of the clock, so the asynch operator lets us be honest about the orders we can't control. Like here's the counter program. Augmented with some agents that can't control the counter. They I can with a up, their messages race to the node holding the counter. The count there's quite a big deal of uncertainty about which agent reads which value. This is a nondeterministic program, we'd want to be to do that. So what did time do? Well, space would appear to be the thing in distributed systems, but space is irrelevant. It's just data and names, right? I can join on it, I can select it but there's no semantics. All of the semantic consequences of distribution are made manifest in the dimension of time.
   Time, you know, distribution can make time unpredictable and unpredictability is sometimes bad.
   But exactly when? Well, to answer when. We need to devise a model theoretic semantics for this crazy new language. now and hope that we can keep it simple.
    Now there is a model theoretic semantics that captures this program.
   The roll of those dice determines a unique model for our program. Problem is it's an infinite-sided die. So we have an infinite number of models for this program, each of which is infinitely large. Infinite number of models, infinitely large, that's not going to help us reason whether thousand program is correct. Like for example I provide this program with some input, I flip a coin and here is one of its stable models. I happen to have picked time 100 for the message to be delivered. Fuck, OK!
   So now we're back where we started from, right? We have a mapping that makes sense, mod. But what about those outcomes? What we really want is to apply that same intuition that we did in the beginning of the talk, right? Most of these behaviors aren't interestingly different. They create data that maybe is different only in its time stamp and we want to be able to group those together into an equivalence class. What we want is to sort of take a magnet and extract from that haystack the facts that at quiescence time have become permanently true and are changing only in their time stamp, the facts that are eventually always true and that looks something like this. Magnet, eventually always true, drop the time stamps. Now we have something that looks like a real meaning of our many practice because what happened in this program? We had a counter initiated at 0. So the it's a unique model despite an infinite space of executions, that's where we want to be.
   Now, some programs, not this one, have a unique ultimate model. They're confluent, no matter what data you give them, for that data there's only one answer. And this would be huge, right? Like if we could, you know, if we could show that a program had only one unique meaning, that would mean that its execution why is deterministic, that would mean that distributed systems were so hard anymore, right? It would mean that we could replicate our computation without coordination, without having to worry about those replicases getting out of synch because if something bad happens it's guaranteed to always happen. This is the holy grail. So talk is over, we win! , actually, no, like many desirable qualities confluence is undecidable and if I had time, which I don't, I would go through Bill Marczak's proof. If you could use a confidence checker, you could use it and we suspect we haven't solved that problem. Which means that confluence is not decidable, OK? So Dedalus added to datalog the ability to talk about what changes, and what changes are out of our control. And it left us with the really big language, a powerful language that could implement any system, a Turing complete language, actually. Too big. We're back where we started from. But it's interesting to consider fragments of that language. That's the sweet spot that's the Goldilocks language. It's Dedalus with no, or with very carefully controlled negation, because  due to the COM theorem we know that monotonic Dedalus programs are deterministic without coordination. We might be concerned with expressivity. But this class of programs is at least as big as the polynomial timed programs, which seems like a good place to be for distributed programming. We don't want to exponentiate over the network, right?  so you can have your state, and your asynchrony and your nondeterminism and you can distribute it, too. As long as you are careful about how you combine. And it leads to a really cool pun. On the word consistent. When a logician talks about consistent they mean free from absurdity. And if you view through the Dedalus lens, it's actually the same idea. So when we commit acts of abstraction, we need to make hard decisions about what to hide, and the hope is that interesting and surprising things will be exposed. With Dedalus, we were very careful to hide a variety of different representations of state, right?
   Like the difference between events, internal events, messages, rosen tables cache contents, we made them all look like rows. We also hit control flow. Because in Dedalus there is no control flow. And that simplified a lot of programming. In return for those sacrifices we got a language that makes three things shine out and be obvious. Data is the fundamental unit of computation. Sometimes we can control how data changes, and sometimes we can't. And we need to know when. And one neat little trick fell out of all this: You can apply old techniques that were never even thought of it would have been applied in a new domain. In particular,  turning the lens some really need stuff fell out, right? Like taking a page from model theory we can look at the meaning, the semantics of the program as just databases and make databases this common language between the programmer laboring over his program and the world of meanings where they go when they ask if their program is correct. And model theory also establishes this amazing connection between the meaning of programs, the uniqueness of a model of your logical statements, and this really valuable systems property of deterministic outcomes despite pervasive nondeterminism in execution. What grows on that? Well, we developed a language called Bloom, threw away that horrible Dedalus syntax but ships with tools that tell programmers whether their programs are deterministic and exactly why they aren't if they aren't. On top of Bloom, we built Blazes which takes that insight a step further. Then I can probably fix it by adding protocol, what kind of protocol? Well, protocol that controls ordering when the ordering matters with respect to the result.
   Now, from proof theory we got another bag of tricks, right? We got this idea that data lineage, something that's easy to collect in something a system like this provides proofs of outcomes. Independent with respect to failure demands, we can prove that our programs are fault-tolerant. The fruit of that would, is a system called lineage driven fault injection. I've spent my summer at Netflix building into a production system using their chaos animals and we hope to take the same off those results soon. What else grows there? Oh, well, the sky's the limit, but one thing I'm particularly interested is eve, a programming environment for nonprogrammers that legitimately threatens to change the way we think about computers and people and their interaction. Watch this thing closely.
   So recall, we wanted a language that makes it easy to write the good programs and easy to know if we succeeded despite the bad things. Dedalus' answer to this is abstract away the bad things, but don't hide the complexity. Use model theory to reason whether our programs are tolerant of unique orderings? Use proof theory to reason whether our programs are robust to failure. What do we want? Dedalus. When do we want it? Eventually!
   [laughter]
   So in closing, some of you might be saying, I don't invent languages. Why do I care about any of this? And I guess what I would say back is, this isn't about inventing languages, this is about committing acts of abstraction, something that we do all the time, right? So my advice about abstraction, flaky as it might be, is first of all we don't do a good enough job respecting our users but a great first step is admitting that we are them. We write these libraries so that we can use them. Let's respect ourselves. Let's respect ourselves and our users enough to not make concrete static abstractions, right? Trust ourselves to let ourselves peer below the facade. There's a lot of complexity down there but we need to engage with that complexity. We need tools that help us engage with the complexity, not a fire blanket, right? Abstraction is going to leak, so make these abstractions fluid. And then finally, you know, I'm a database guy, so I'm kind of guarantee-centric. But give me a choice between a nice syntax and a nice semantics, something that feels good, but means well I'm going to choose means well every single time.
   In the interest of coming clean, it is about inventing languages.
   [laughter]
   Inventing languages is really fun, and so I know a great place, so if any of you are interested in spending the next five or six years screwing around, inventing languages, building systems with them, studying the systems with the systems, studying the language with the systems, I'm looking for Ph.D. students, come join me. Thanks.
   [applause]
   >> Two minutes if you want to take questions.
   >> Sure. A two-minute question? One question?
   >> I have a 30-second question. About eve, --
   >> Really? So is Chris Granger in the house?
   >> Yup. Talk to that guy. Ooh, that was easy.
   [laughter]
   >>
   >> Peter, what is the behind every API is a programming language, there's an old saying from the time before, right?
   >> Sure.
   >> So is there really a conflict? The last slide is --
   >> No, I think I was obviously contradicting myself in the last slide when I said it was and wasn't about inventing language, but I hope that the message that resonates is that these acts of abstraction that we commit every day from defining function to defining a library, to defining API, to defining a language in all different points in the continuum it forces us to make challenges.
   >> I can't quite hear you. Could you speak up a little bit?
   >> You chose -- [inaudible] in your logic, have you -- substructural logic yourself?
   >> I have not considered substructural logic, but I would love to learn more about it and talk with you about it.
   [applause]
   >> All right, everybody, we'll be getting started in about 10 minutes in all the ballrooms and in here. So find your next place of awesomeness and go there.
   
