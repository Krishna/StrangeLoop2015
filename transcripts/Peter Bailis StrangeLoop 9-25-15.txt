   
   Peter Bailis: "When 'Worst' Is Best (In Distributed Systems)"
   
   [Live captioning by Norma Miller @whitecoatcapxg]
   
   >> All right, if the mic -- mic check, super. I guess I'm introducing myself. Thanks all for coming after lunch, I'll try to make this entertaining and exciting so that you don't fall into a food coma, but I won't hold it against you if you do. Today I want to talk about a somewhat crazy question when we're building systems, which is what would it mean if we designed computer systems to handle the absolute worst case scenarios? 
   So for instance, if you were provisioning a cluster of distributed services, let's say you're provisioning hardware for your startup very reasonably you're going to reason out questions questions like how how bad is other load going to be in the worst case scenario and if we take this question to the extreme, like what is the absolute worst case our service might be up against? You know, in the worst case if you're essentially building an internet, you might have to handle 7.3 billion simultaneous users. That is, everyone on planet earth logging into your site at 100%. Now, you could probably buy enough hardware but in all likelihood in fact for every application today, even Facebook we're going to end up with a bunch of idle resources if we in fact provision for this crazy absolute load. Now, similarly if we're trying to build hardware, this laptop is actually built for you know, I put this laptop through a fair bit of abuse. In reality if we want to ask, how could we build a piece of hardware that would stand for absolute worst-of-case environmental conditions, we might say, well what would happen if we put this laptop on the Mars rover? Well, some people do build chips for interstellar space travel, the fact is if you want to build chips that travel in space it ends up costing hundreds of thousands of dollars to protect for things like radiation. So we could build chips, that in fact worked in all of our laptops that if we sent our laptops to Mars, they would work, but for the average case sitting here in St. Louis, Missouri on earth, we've end up paying a lot more for packaging that we don't end up use being. We care a lot about security. We hear about security breaches. It's actually a fairly important problem today. But especially when we talk about security, we can get paranoid. I stead of worrying about people getting into our systems, we might worry what happens if our develop why er team turns against us? Now, this is like a particularly crazy question. It turns out there's some really cool research on this project but not surprisingly actually handling this worst case ends up really increasing the cost of both the deployment and development of the code. Not surprising, right, by answering this question we find that freakily designing for the absolute worst case ends up penalizing the average case. We end up making tradeoffs that essentially hamper our performance when bad things don't actually you are can. So in terms of this tradeoff, we might visualize it as a grapher. When everything is going fine and we haven't hit sort of the more and more the more and more we start to peoplize ourselves, and this actually happens a lot of the time. For instance, even ...
   You're going to be able to move quickly and not get too hot. Of course there is some possibility that there's a freak snowstorm that lasts for a year and you're trapped outside in your shirt and shorts.
   The problem here is that if you really wanted to guard against that worst case scenario you'd probably have to put on a winter parka, might have to bring food for a year, bring a portable camping stove, very reasonably you wouldn't be running outside but you'd be trudging with this sort of dooms day resource, clothing and so on. Owe the question is if this is sort of a dismal tradeoff where the more sorts of worst case behaviors and environmental conditions that we want to account for, the worse we perform sore the of in the average case when bad things don't happen. In this talk I want to ask what would it mean to get the best of both worlds, where by designing for the worst case, we can actually improve the average case, as well. This doesn't always apply, but in this talk I want to highlight when it can apply and what it means in the way in which we think and build software systems. in this talk I'm going to describe three major points. Now, it turns out a couple of other instances we found where in fact this tradeoff isn't really a tradeoff, at all. Where designing for the worst case actually improves the average case and this has pretty interesting implications for general optimization problems. This type of reasoning, not in your distributed systems, but also when you're building software systems in general.
   So to kick things off, let's talk about distributed systems in the network. I think distributed systems really matter. So almost every application today that is essentially nontrivial is or is either becoming distributed. So if you want to build a rich application today there's a good chance you have delinquencies on external data sources. You have APIs that you end up requesting when you build a mashup. You have a large scale services that's comprised of a large services. Or you have a server where no one component plays all the different roles in your application and realistically there's some point in your application where you're opening a socket. Now, the core -- you probably are opening a socket. So what happens when you open a socket, right? What's the defining characteristic of distributed programs it's that they operate on a network. There's some communication medium that's more than just say invoking the call.
   So it turns out that networks if you built distributed systems which I imagine you have, networks actually make designing programs very hard. So a lot of things can go wrong. First of all, networks can delay messages or packets, so if I send a message to you, it might very well be the case that we have a latency of a million second or a half millisecond but as soon as there's congestion in the network then my latency becomes unpredictable. And effectively for modern production systems we have to imagine the possibility that packets may be delayed arbitrarily long, especially if we're deploying on modern cloud computing structure like Amazon. Now, if we dial up latency or these delays to infinity, we essentially have network failures, so because our networks communicate over physical media, right, over either, you know, wires in our data centers, or over wireless signals as we do with your WiFi or your phone, we have to account for the possibility that networks may be unavailable, that packets may be entirely dropped on the floor. In fact, in distributed systems, we can actually capture both of these behaviors in something that's called an asynchronous network model where I can't tell if I'm missing a message whether the message has been delayed or lost.
   And this makes things very challenging.
   The way in which we'd like to handle this asynchronous network behavior is very skimp which is let's not rely on the network at all. So if we provide what's often known as availability, we can build systems where replica, any. So the: In fact, under this definition of availability, any partitioning of the net, would, any network behavior that might arise, for instance if these two servers can't communicate, left and right continue to service requests independent of one No. another. Similarly if our server on the right for instance fails the server on the left continues to serve requests essentially nothing has happened. So this is a very powerful idea, right, if we can achieve this availability, we essentially don't have to worry about network failures. These delays and drops become a nonissue terms of our systems design. Notably we've been talking about this in terms of failures, right? However, this notion of availability applies also when these servers can talk. For instance, if our systems available, even when the network is fine, we still don't have to exchange messages in order to process requests, which is very powerful. In being if a we say that there's no coordination between our servers and you are 0 system. So availability addresses this worst-case scenario. If we don't have to talk, then the network can do whatever it wants, and we're fine.
   However, if we can talk, and we have this availability property, we still don't have to talk. Right? We don't have to make use of the network and this leads to several important benefits that correspond to average-case benefits.
   >> Specifically if we don't have to talk, if we've built one of these coordination-free systems, first of Paul we can scale out our system potentially indefinitely. Fairly straightforward. If I throw more resources the a the problem, let's say I provision 100 easy 2 instances that run my processes all of these new instances can start taking requests without communicating with the previously running servers.
   This is a pretty huge benefit. Now we get essentially guaranteed linear scalability just because as we add more servers, they can start processing requests without affecting load on the rest of the system. In contrast, let's say we chose a coordinated mechanism, we elected one of these nodes a primary. Well, any request that would have tock to the primary wouldn't benefit whatsoever from adding these additional resources. This seems almost trivial, but it doesn't come for free. We have to explicitly design for coordination-free execution in order to guarantee that no matter what happens, independent of data layout, independent of request patterns and so on, we can effectively throw more resources to the problem and get more performance. Now, what I just described here has to do with add are Mo are resources. What about making use of the resources we have? Well, if we want to run say something like distributed transactions and here I'm going to show numbers Amazon EC2. We can plot the number of servers we touch per transaction on the X axis and the through-put or the number of operations we perform every second on the Y axis.
   So when when one item per transaction I'm going to touch server A and item A. If I have two items for transactions, I I'm going to choose two servers.
   Not surprisingly if I coordinate if I have something like in-memory looking where I want to touch A and D, I perform my practices operations and I release the locks it's going to be fairly slow. Note this is a long scale on the Y axis, we have a huge penalty when we start performing multi-server transactions. Specifically almost a 400X decrease in through-put whenever I'm touching B and you want to access B, but you have to wait for my lock it's going to be very slow. Essentially we're bounded which how quickly we can communicate. Not surprisingly if we don't use coordination, let's say for the purposes of this workload we don't grab any locks, I'm touching B, I'm waiting to go to D, you come and touch B, you perform your operations and you're allowed to run concurrently we don't have this through-put hit at all. Notably, even, I said this talk is about distributed systems but in fact there are benefits even on a single node sometimes. Even if we have one transaction on the far left of this graph we see the coordination free case gets better in order of magnitude. What's happening here is we're running this on a server with 16 cores and 32 hardware threads. In being if a be with the coordinated case, all the threads queue up on waiting for their chance. So these coordination free systems can in fact improve through putt often considerably. And the key here is that if I don't have to wait for you. If I can put in my operations independently of what you're running on the data, then we can each run at the same time.
   Now, a rerelated performance related concern is latency. So given that our servers and our processes are actually deployed on planet earth there's an upper bound to how quickly we can communicate, namely provided we use sort of standard networking technology and we don't communicate faster than the speed of light the maximum I'm sorry minimum latency after chief is 133 milliseconds. Now, if we get very clever and we try to for instance drill through the center of the earth like high frequency traders might like us to do, the minimum latency we can achieve is 85 milliseconds, and again, if we're accumulating all our questions one after the offer, the maximum latency is 85.1. This is not very good.
   >> So these coordination free systems give us these benefits in enabling infinite scale out by improving through put and ensuring low latency all in this failure-free case. I haven't mentioned the word fails whatsoever. It turns out that yes, if we do have failures here, then we will actually guarantee an always-on response, that is, by providing availability, that is No. 4 here, we in fact get 1 through 3, as well, but it's not -- this doesn't just apply to the failure setting, OK now, many of you are probably sitting in your seats and saying, gosh, what about the CAP theorem? Aren't there some tradeoffs you art telling me about. For those of you who aren't familiar with the CAP theorem system. Brewer along with his students in the 1990s, worked on a search engines called ink tomi.
   You're going to have to make hard tradeoffs in you want to make sure that one of insection servers is not down, you get a 404. You'll have to trade off between availability and always giving the right answer or the correct answer. In fact there's a tradeoff and there's a very nice paper in what's called harvest and yield that talks about trading off, for instance, querying all of your index servers versus querying those that are availability at the time of query. If you want to get the absolute right answer, you'll need to wait potentially in the event of failures. The take away from the CAP theorem is fairly simple. It's that certain strong properties require unavailability or in other words require coordination. For instance if I'm on replica 1 and you're on replica 2 and we have a single data item X. If we want to make sure that I see your rights, you will have to be unavailable. Specifically if you update X equals 1 on your replica and I read from my replica, there's no way I can read your write, unless our replicas communicate. It's as simple as that, and in fact for a fun afternoon read, I suggest you actually look at the paper proving the CAP theorem that actually makes this he can L. exactly this argument. We need to communicate if we need to share state.
   >> Now, this is sort of dismal. We had all these sort of nice properties we got from our coordination-free system. "H. the question is, did any of that matter? This is a very common sort of incorrect conclusion, that I see a lot. When people talk about tradeoff for availability and consistency and so on, the line goes something like availability is too expensive. We have to give up all of these nice properties, it om matters during failures, so who cares?
   Well, it would be sort of sad if that was the end of the story. And perhaps surprisingly is that many guarantees that we actually depend on today in today's systems don't actually require giving up availability. In fact if we treat this worst case systems analysis as a design tool, we end up expanding the space of algorithm protocols we use to build these systems. Today's databases in particular are designed for a very tightly coupled cluster of servers and they inherently use coordination. I'll go through an example in the next slide.
   However, coming back to the theme of this talk, what's the worst that could happen? When we pose as a research question, what if we built systems that didn't have to coordinate, that weren't allowed to coordinate? Right, by pushing open that spectrum within the space of possible distributed systems design, we found that in fact in many cases we can build designs that avoid coordination unless specifically necessarily. And moreover coordination free implementations are actually possible. So let's go through a simple example about asking what's the worst that could happen? The example that I'll give is called read-committed isolation. The fancy name for a very simple property, the slightly simplified goal is in a we'll never read uncommitted data. So if I write X equals 1 and X equals 2, you'll never read X equals 2. you will. It ounce about that every database system today implements this guarantee. In fact it dates back to 1976, when Jim Gray who won the Turing award actually thought about what it meant to provide recommitted and in Gray's original formulation he used coordination to implement this guarantee, specifically since 1976, most databases have implemented read committed by simply grabbing locks. Now, this is correct. The problem is as we saw earlier, holding those locks is fairly slow. On the single node systems Jim Gray cared about. The today the network is the slow thing and these types of implementations no longer scale. Coming back to our question, well, how can we build a system that didn't coordinate? Moreover, is coordination strictly necessary? It turns out we might be able to do a little better.
   So how will we avoid coordinating using these locks?
   >> Well, one possibility, and there are several, is for instance, to use multi-versioning. For example, if I have a private register, X prime, that I write to instead of updating the primary copy of the database. And then once I commit my transaction I copy X prime over to the real X, you'll never see my intermediate state. This is a very, very simple limitation. It's as simple as it sounds. In fact there are more high performance implementations of this that we can consider, but even this simple implementation can offer gains over these classic implementations, despite the exact same semantics.
   So what this highlights again is that by focusing on this worst case, by asking how do we build systems that under no circumstances coordinate, we actually deliver a large number of benefits in practice, and accounting for this worst case in terms of network behavior actually ends up improving the average case in terms of performance and scalability of the systems when we deploy them. The punchline here is systems that behavior well during network faults actually behave better in cases where there are no faults. And in good designs where we've explicitly accounted for the worst case we've built systems from the ground up with the assumption that bad things are going to happen, we can in fact benefit legacy implementation. In fact, Martin Kleppmann is going to have a talk tomorrow that I encourage you to check out where he goes into a little more detail in these topics why.
   >> We can frequently provide meaningful guarantees that occupy really a different corner of the design space than traditional systems thought of doing at all. #.
   >> So I hope I've convinced you that distributed systems have a lot to Ben knit terms of this worst case thinking. When I was preparing this talk, I was surprised to learn that there were a number of other situations under which this type of thinking similarly benefit us. I want to highlight some instances when I polled some colleagues.
   >> The first is talking about replication for fault tolerance. So if I have a database system for instance or I have a stateful service and I want to make sure that when one of my I stances goes down I don't lose data, I very well might end up replicating this service, right. So I might have multiple copies. By replicating in order to not lose data for many operations I actually increase the capacity that my system can handle at any given time, as well, for instance if I have a primary backup I can sort read-only queries for my backups.
   So another scenario. Typically or I hope in many of deployments, failover is handled. If I have sort of machinery to automatically fail over my primary where when the primary goes down I elect a secondary or I elect a replacement, then I can use this same failover mechanism for a bunch of cases that aren't failures at all.
   Simple example, let's say uh want to upgrade your database from version 9 to version 9.2 of the software, if you didn't have automatic failover, you might turn off your front end services, go through each of your database servers and then upgrade them and once you've made your upgrade, you turn over your front ends and traffic resumes. You simply have a barrier and a lot of availability in order to perform a routine service operation.
   In contrast you can start killing them one by one, kill a server, upgrade its software, turn it back on. If you have automated failover in your clusters, the nodes that when they're down will essentially have their work migrated to other servers within the service. And for instance when you have a slowed node in something like Hadupe or spark you can use the same tools to migrate stateful tasks from one server to another, that will improve the through-put despite there's no failure. In fact when you're running pa cluster manager like Masus, you tell your application team, sorry we need to recommission this node because a new service has higher priority, you don't have to go through the manual process of notifying them necessarily. So this mechanism that comes into play only when there are rolling outages or when we actually have are server failures can be exploited to your benefit when you're performing common operational roles.
   Let's talk about micro-ser services. Micro-services are very cool. Or you can talk about service oriented architecture. If you think about a typical service, it can have some distribution of latencies, so in this service here, it's sort of a fictional micro-service, this is your service. Let's say that 99.9 # 9th percentile latency is 100 milliseconds. So 99.99% of your requests take 100 milliseconds. This leads to an average latency of 1.2 milliseconds. So you say how do I bring this down? Let's say you get an order of magnitude improvement in your tail latency here. Under this distribution here for your average latency you'll see less than 10% decrease in latency. So really you're putting if you put in a lot of work you get order of magnitude reduction in tail latency, you're seeing it in most of your requests. However, this is just one service and chances are you're running a service-oriented architecture, service is part of a much more patchwork set of services that are essentially orchestrating the run time of a larger application.
   Specifically, right, you might be one of say hundreds of servers that are being queried at once in order to build a front-end sort of HTTP request and we imagine you have 100 of those servers here that they're being queried by some front end, then your tail latency is very like lip to appear on average for every request. So the average front end latency with 100 fanout for this distribution is around 64 milliseconds. In fact now in this case if you were to do the same exact reduction, reduce your tail latency by a factor of 10, the front end average latency which previously only reduced by less than 10%, you know, essentially sees a ten fold increase. What this demonstrates is that your service's corner case in fact may be the average case for the consumers of your service. And this is simply a matter of statistics. The more requests we draw from a distribution, the more likely we are to hit outliers in that distribution.
   OK. So these examples are all other instances and sort of large-scale systems design. It turns out when I talked to some colleagues from human computer interaction it turns out there's an interesting application of sort of worst is best in what's known as universal design. So the idea between universal design is very interesting. This picture here is a cush cut it's those things on the sidewalk instead of tapping down 6 inches, you curbcuts were originally designed to not have people in wheelchairs and sort of wheel themselves up a 6-inch wall in order to get onto the curb. now, what happened when they designed these curbcuts for wheelchair easers is that people like you and me or those of us who aren't actually in wheelchairs came to benefit, as well. For instance, people riding bikes can use curbcuts, I use curbcuts when I walk across the street, as well. This is the classic instance of the universal design that says by building systems that accommodate everyone who might be using a sidewalk we end up benefiting not just those people for whom the system was originally designed, but we benefit everyone else.
   Similarly when I watch Netflix, I frequently turn on the subtitles, and I imagine that a number of people in the audience who turn on subtitles, is much much larger than the number of people who necessarily need subtitles in order to absorb the content. However, by reasoning about it and building in different ways to absorb this video content, we actually improve the user experience for everyone.
   So this idea behind universal design is very important for accessibility but it has benefits that transcend simply the corner cases or the outliers in terms of the users who might actually access our services. For instance, the W3C makes a very interesting case that there's a strong business case behind accessibilities. There's a encloses correlation between the design practices that improve accessibility and design practices that improve things like mobile web design, usability and even things like SEO. So by handling sort of design for everyone, we in fact improve the experience for everyone. It's not simply that we're reasoning about outliers, it's that by sort of changing -- by considering a larger scope of users, we end up improving the design experience for not just the people on the fringes, but for more or less everyone involved.
   One final example. It turns out that actually simply best is not always best. So if we're trying to maximize some sort of function as we might do in optimization here, we have F of X which is cut off on the Y axis. It's eeasy to choose the maximum situation. Now, this is an idealized function. What happens is that in the real world, functions aren't always so ideal. For instance if we have a function that looks like this, we're going to have the same globally optimal point in the middle here, but only if we can very accurately choose the value of X. That is, this solution is fairly brittle in that if we misestimate our parameters, we might very well miss the target and end up with a solution that's much worse, in being if a, the worst possible point on the graph. Instead what we might do is try to opt for what we call a stable or robust solution, in fact sometimes choosing the absolute best is knot actually best unless we're extremely sure that we've hit the right mark hon the X axis and there's a whole body-of-literature that studies finding the stable solution. In fact, in real-world optimization problems, for instance, trying to decide, should we, you know, how do we route, for instance, taxicabs or how should we deliver packages, very frequently we see functions that look like those on the right as opposed to those on the left.
   So I hope I've convinced you that designing for the worst case can in fact improve the average case and I think there are some opportunities to apply this type of thinking in your designs going forward, as well. Specifically this type of thinking doesn't always apply. As we saw at the start of the talk, there are a couple of instances where thinking about the absolute worst case was not a good thing, however, in your application, if corner cases are quite common or similarly unpredictable it might be worth thinking about what the worst case means for your operating environment and what it means for your application when things you didn't think were going to go wrong did go wrong? A nut shell, maybe this is a bit tautological, but normally thinking about what is the worst case is a remarkably powerful tool because the way we define normal, the way in which we say this is expected and this is unexpected end up having a profound imfact Pacht on the way we build systems. In fact, even going through -P the exercise can be very instructive in helping us to find points in the design space we might not otherwise have thought about. We might not otherwise have curbcuts, we might not have gosh, what happens if we build systems that don't ever have to talk. It sounds ridiculous, but it's very helpful and pushes us towards new spaces in the design space. Coming back to our earlier examples we saw that in cluster provisioning it was ridiculous to try to cluster a provision for 7 million users. But to ask this question we might not to need worry about 7 million users, but what we might ask about what's our strategy for scaleout? This sort of worst-case thinking puts us on a trajectory that we should maybe not go to the far logical extremes of the conclusion, but we can kind of think, well, gosh, what happens between here and 7.3 billion users? Similarly in terms of hard war, yeah, we probably won't be sending MacBooks to Mars any time soon, but it is good for us to asking what happens if we have bit flips. Where's the right point in that tradeoff space? In terms of security, you are she, yeah, most likely all of our developers aren't trying to compromise our data but we should probably have a strategy in place that allows us to manage or audit internal data access so by thinking about these extreme cases we make sure we have drawn the boundaries in the right place. Every system has to draw some kind of boundary. It's unlikely that -- but knowing which scenarios we care about, and which scenarios we've opted to not sort of accommodate, is extremely important.
   I think at a very, very high level, thinking about the worst case is a very important tool for essentially examining your biases when you're building software and you're building systems. Who is this software designed for? What environment do we expect this to be running in? And what happens if we're wrong?
   So in conclusion, reasoning out worst case scenarios can be a very powerful design tool. In fact, this can improve performance androbustness for a number of other scenarios when we build and operate software in practice. So I'm Peter Bailis, there are a number of people who helped out with this talk and I'd love to take any questions. Thanks for your time.
   [applause]
   >> Don't have time for questions. Oh, out in the hall. Thanks ... ... ... ...
   [break]
