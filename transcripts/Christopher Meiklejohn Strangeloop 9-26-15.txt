   Christopher Meiklejohn: "Distributed, Eventually Consistent Microsystems"
   
   [Live captioning by Norma Miller @whitecoatcapxg]
   
 >>All right, I'll start. Hello? Hi, I've got a tape I'd like to play. Consider a distributed register, so we're going to have two copies of this distributed register replica A and replica B. We can do two operations on this register, we can set the value and retrieve the value so here we're going to set the value of 1 and we're going to asynchronous send to replica B.
   >> And we can also set the register to C on replica B. So once we deliver these messages in the systems how do we know what the value is going to be? How do we know that replica A and replica B are so traditionally we kind of use synchronization in systems to especially force an order and what this allows us to do is reason about what the value will be given certain inputs, right? So given that we take certain actions we can reason about what the result of those actions is going to be. So this assists in making programming easier. This also eliminates accidental nondeterminism in the system.
   >> You can have the value under multiple executions of this for the same input return different values for the output.
   And kind of traditionally we've had techniques for doing this up is as locking, mutexes, so synchronization is expensive. We have to spend time to wait for things, we have to spend time to order things. In some systems that's OK. It's not great, we'd like to not have it, but there are kind of new two trendy classes of applications that make this problem exacerbated, right? So the first is, internet of things. So if we think about what internet of things really means, we have devices that are low power, they have limited memory to operate with,  and they usually have limited connectivity and the reason they have limited connectivity is because it's really expensive in terms of power to run the antenna that a lot of these devices use. In these devices happen to operate over shared state and shared aggregates, they can come online and we'd have a problem of divergence. Similarly, one of the research partners in the research group I'm working in is at roving entertainment so mobile gaming is another place where this is difficult. For instance, Angry Birds has leader boards, has profiles and wants the ability that people with continue to play the game if you're offline, like if you're in an airplane so if you have this you're going to be modifying the leader board, updating potentially the profile, maybe you changed the photo, maybe you changed your name and if there are multiple people playing under the same account you have the problem of reconciling those concurrent changes that happened while clients were offline and with things like leader boards even when the state is coordinated it's 1998 that we all kind of contribute to, we have the problem of saying how do I manage this in into this global kind of leader word aggregate.
   >> But we also want to use shared state, right, because we like this, we want to have concurrent edits, we don't want to route everything there you a single kind of client to perform an update. So we want the best of both words. So the idea that we're working on for our research is trying to start from the point of zero synchronization. Can we start with no synchronization at all in the programming model and then only apply synchronization where it's necessary? So instead of you know, build instead of building an application ecosystem around a strongly coordinated system, such as Zookeeper, why don't we design systems that start with no coordination and only use the coordination mechanisms when we need to enforce something, a global system invariant?
   So the goal of the research that we're doing and that we've done over the past three years and are continuing this year into next is we want to explore the limits. We want to see, can we design a computational model that allows us to write coordination-free computations and make sure that these computations are free from concurrency anomalies and when when I say concurrency anomalies I mean like the Amazon dynamo shopping cart. We have on the Y axis here the difference between a strong sharing property and a weak sharing property. On the X axis we have coupling as an infrastructure property and we're not talking about data center specifically we're talking about data center kind of latency guarantees moving to open internet guarantees was as we move outward. So things like map reduce kind of fit into the weak sharing model and the strong coupling model, right? So it is possible to do map reduce across the world I imagine, you can do it, but usually you want to run it inside of a data center, right? You need ha strongly consistent kind of coordinate membership service and typically you only share state at the reduce space. So this is embarrassingly parallel. We don't share any data, you kind of contact the service every once in a while to check out a block. If people compute multiple blocks they don't care, because that's just more information. So SETI@home is a weak idea.
   So if we have an idea tied to this idea, it's like Twitter. I imagine that timelines are kind of objects that are contended for that we have concurrent operations on, right? Your friends are going to tweet and they're a going to appear in your timeline so we have a single source of truth of this object and we're going to write to this object so this is an idea of strong sharing so in an graph like this that you would see in an academic talk? Where do we want to go? Well obviously always in the upper right hand quadrant because that's the area that usually never has anything in it and this is the area where we get to have a weak coupling to the infrastructure where we don't rely on a different distribution model or latency requirements and we can tolerate things being offline and we want to move towards the strong sharing model so we can share a lot of data between a lot of different clients in the system.
   >> I'm keep this short because there's so many distributed computing talks here that probably reiterate a lot of this.
   So when we look at concurrent programming, concurrent programming had a problem of consistency, right? We have to reason about what can be seen by what threads in the system and the ideas of how we maintain caches and things like that. But when we introduce the problem of partial failure, right? We're going to write to a bunch of replicas, and some of them are going to succeed and some of them are going to fail and then we're like hm, OK, how do I reason if I go to contact the system again and get a value that I wrote, will it actually be there again?
   A lot of modern databases do this day, we do distributed computing assuming the illusion of a single system and this is something that you see in transactions, right? Transactions are an idea to address this. And the idea of the systems -- the single system image is that if I have a distributed database and I write a value to one of the replicas, and for instance Tom goes to read that value, he will see it as soon as my operation completes, right? This idea that we enforce it's like a single system. It's like there's only one copy of the register even though it's replicated everywhere.
   So if you attended Caitie's talk yesterday she talked about the idea of consistency models. So consistency models are in what it says is that if you follow these rules I guarantee that you'll see this these events in a particular order and some don't make a guarantee, right? And that's our favorite consistency models, right? So these consistency models exist on a spectrum, right? It's weird the graph is all over the place and all of these things and you have snapshot isolation and nonmonotonic snapshot and all of these crazy consistency models and they kind of all exist on a spectrum. They're kind of partially ordered I guess, right? And to get a stronger consistency models as we move towards a stronger consistency model, we need to enforce this model through the use of synchronization, rights so if we need everybody to agree on a value immediately at a very particular time, we have to use coordination to do this.
   So what we want to think about is that a consistency model is analogous to a programming paradigm. Because it's a contract. If you think of a programming paradigm like, you know, functional programming or data flow programming, this is a contract that says well, given a program you follow these rules, you will see these particular types of outcomes, right? So it's a contract that the application developer makes with the underlying system that they're building their application on top of.
   So what we'll kind of step back and say, all right, why is synchronization undesirable? So it's kind of intuitive that you don't want to do this but we'll kind of walk through it anyway. So we have this idea of like physical time is really hard, so if you went to John Moore's talk yesterday he talked about the problem of synchronizing computers with NTP, you know, how at certain interval the computer gives up, some computers won't even synchronize. We talk about the problem of time stamps when they're used for deletions and if you delete something in the future, what does that mean? So that sounds like it's a big problem.
   And you know, just generally using physical time and applications introduces complexity. It's a hard thing to work with.
   So time kind of takes three manifestations in computing. The first one is kind of mutable state and sequential systems, so if you think of sequential systems and you have functions, and these functions use mutable state, these functions are going to take different values over time. So that's kind of a difficult thing to think about, right? We don't have these nice properties of referential transparency and things like that that we really like. 
   Concurrent systems we have nondeterminism. We know that we can execute at multiple times and get the same result it's the worst thing that you run a computer program and three times in a day and get different answers. In distributed systems we have all these problems of network latency, right? How do we do failure detection? It's a fundamental problem because it's very hard to do in a safe manner. How do we coordinate, right do we ensure that we see events in a particular or the? So a lot of this is that we have to deal with time because it takes time for messages to go across the network. But ultimately physical time is unavoidable because it's kind of like a real thing, right? When you sit down at a computer and you punch like you know, numbers if you have a distributed word counting application and you're going to punch some words in to count them, you know, you've going to punch those words in at a particular time and you're going to read the result of the words that were counted at a particular time, right? So physical time is unavoidable. It's essential.
   So so we'll digress for a minute and talk about the parable of the car. So we think about a car and how a car drives down a highway, and so as a car drives down a highway, it uses -- the tires grip the road, right? The tires grip the road and this creates friction and this allows the car to propel itself forward and these friction points that the car makes with the road are very small, right? Like the car has a massive surface space but you have these very small contact points where the car makes contact with the road, where this reaction happens.
   But you know, if you have a motor in that car, then that motor really doesn't like friction. You don't want friction anywhere near that car.  So the motor really relies on it being as frictionless as possible. And we all know that when there's a lot of friction inside of a motor, lots of bad things happen, your system slows down, your system stops functioning correctly and these are all things that cause the system to be very difficult to work with.
   So we're going to kind of think about physical time as friction, so we cannot eliminate physical friction to the problem because it's essential but we want to reduce physical time in our programs as much as possible.
   So if we look at this box here this box is a data flow graph so just imagine it's some general data flow computation happening inside the box and we want to push physical points with those interaction times, as the car ininteracts with the physical world like the ground and the highway we want to push time out. We don't want to have physical time inside the box. But it's OK to have a notion of logical time inside the box and we do this all the time. If you saw John's talk he talked about the use ever lamb port clocks and we have all of these causal tracking mechanisms that allow us to reason about how events are propagating in the system but we don't do this based on physical time. So you might ask, well, why do I care if there's time in the box, right?
   Well, I mean the reason you care is because you're the programmer who's writing the box and that's a difficult thing. You don't want to have a difficult task where you have to write all of this code based on time.
   So, going back to the second slide where we said, well, what can we do with zero synchronization? We can't really do much. If we have a bunch of computers in a network and these computers are not actually talking to each other, you know, they're probably not working together and they can probably just be independent single systems.
   So we can't really do much without synchronization at all.
   But a really interesting kind much middle ground is that we can look at this property known as strong eventual consistency and what this is is it's eventual consistency model that we're familiar with, about you with a strong Von vergence property and what this case is that replicas that deliver the same updates in any order and can be different orders in any node will have an equivalent state.
   The primary requirement for this model is fairly weak, it only allows eventual replica to replica communication, and this can be done transitively, right? We don't actually have to have every node talk to every node.
   And what what's nice about this is it's order insensitive so it's commutative. We can receive the updates in any order and it's duplicate insensitive. It's also insensitive, as well. That's a really good match for unreliable asynchronous networks, our favorite kind of networks.
   We're going to briefly talk about monotonicity, we don't have time, I can't pull up Wikipedia and walk through it with you as much as I'd like to, but you know, we will kind of do a brief demonstration of what monotonicity is. So if we go back to our original example where we have replica A and replica B, we do our set and we propagate the result. And then we write 2 and 3. So now we have a decision to make so if we're operating over an ordered domain of which the natural numbers are, so let's assume we only set natural numbers, this domain is ordered then we can have a monotonic function which is going to preserve that order. And this is going to give us deterministic execution. If we use something like max which is a monotonic function over the natural numbers so here we see we get 3, because 3 is the greatest value that's been seen and I can take any node on this graph and I can completely rearrange it and still get the same result.
   So if you went to the CRDT talk that was earlier you probably already learned about strong eventual consistency so I apologize in any duplication in content. So how can we succeed programming strong eventual consistency? So we'll we will we'll start here actually. So we'll begin by eliminating scental nondeterminism so we've seen that these data structures in these monotonic they're really helpful what we have to do is we have to if we want to make this general, we have to eliminate, we have to have the ability to model nonmonotonicity in a monotonic way. So if you think about a set, a set isn't monotonic that you can remove an ad from, because over time the cardinality is changing over time. So we have to be able to reason about it in a delivery of events in our system. We'd like to retain properties of functional programming that allow us to compose those objects because it's been shown that convergent objects if I just take a convergent counter and shove it in a convergent set that will not converge correctly. These are difficult things to do. So we want to move kind of closely to properties that resemble confluence and reverence transparency and finally we need a way to distribute all this stuff. We need a way to run it in a fault-tolerant run time and be resilient to failures and highly available and all of the things that make it very practical to use something like this in production.
   So we'll start by eliminating accidental nondeterminism. So you probably heard about CRDTs, I think there's been a talk at every sing Strangeloop on CRDTs.
   And these types exist with many different properties, we have different varieties of all these things, sets, counters, flags, which are like billions. We have different type of registers, we have a map that allows composition for a single data structure and there are graphs. And these data structures try to mimic the sequential counterpart as much as possible, however a sequential version of a set doesn't have a concurrent. So a lot of these data structures they take biases they say under. What we'll do is we'll bias towards adds because we intuit that users will only remove things that they've seen in the set. So if you follow these rules again it's back to this idea of a contract, right, if you follow these rules, you get these guarantees.
   And what CRDTs do is they strong strong eventual consistency through object.
   We're going to walk through an example. replicas A,B, and C and we have a value, we have a sept of unique constants representing unique additions like amounts and then we have a set of removals. So when I add one to replica A, generates this random lower case a constant and asynchronously propagates. If replica C adds one, it hasn't observed A's yet so replica C will add with unique constant C and. Now, if C wants to remove one from the set, so inside the circle is the user queriable -P from the set and what we've done is we've created as a tombstone set and this tombstone set tracks the removals of items in the network so now when we deliver all of these messages eventually we guarantee we converged the right result and the it's a pair Ys merge over the triples where the first element matches by unioning the set and union for these sets, because they only ever grow is monotonic.
   OK? To determine if the element is in the set, we take the difference between the add set and the remove set and if that is not the empty set, we remove it. So you might be looking at this, saying holy crap, that is a lot of data, you are storing a lot of data for a set that at one time had no elements in it. And yes you're absolutely correct. These sets are modeled, you know, these are the complex complexity is the size of the operations, but there is actually a very efficient modification that you can make to reduce this to be just the size of the active elements in the set with a vector that represents the actors in the system and how many times they performed updates, so there are plenty of ways to optimize this, but those are very difficult to explain in two minutes so I did not use them.
   So now that we have these data structures if we want to kind of retain the properties of functional programming like these ideas of confluence and referentialal transparency.
   So this is the lattice processing work, so this is Lasp, I don't know, you might have heard of this is the stuff I've been doing for the past year and this is a distributed deterministic data flow program. And what we mean by that is if the program output is CRDT even if nodes get partitioned away and eventually come back online once I deliver all the updates I will get the actual same answer at that node. So all of the nodes will converge at the correct value. We build this by building on. We have lattices that we use as the primary data abstraction. So we don't have normal -- you know, like an integer, so there's not an integer.
   And what we do is we have a way of doing composition that all of that internal metadata that I showed you in the previous example we have a way to use functional composition and map that metadata through and build other CRDTs that are based on other ones, and last in Erlang and Erlang syntax is a little bit debatable sometimes. So I have a little bit of pseudocode in here. Erlang's type system doesn't really allow for extension, and what we have here is we declare a set of and we call it S1. We're going to add three items to the set. So we add items 1, 2, and 3, and then we can declare a second set and then we can take the first set, apply a function to all the elements in the set and generate a new one. So this S2 is going to be generating a CRDTs and it will be constantly updated. So lattice allows functional and set theoretic operations on set. We specifically in the paper focus on sets but the implementation has stuff for counters and stuff like that, we allow, you know, we have operations that look very familiar, product intersection, union, filter, map, fold, things like that.
   And what these operations do is they do the metadata computation, they transform that metadata, given the function. So if you have a map and you map into another set and I change the value of the element, it maps the metadata through, ensuring you have a CRDT on the other side. If you have a fold and you fold a counter -- a set into a counter, it will perform that metadata emergency so we have this idea that we need to conserve all of the metadata across the objects to guarantee convergence. So kind of the way lattice works we'll do a high level overview is that each replica is a monotonic stream of states so each CRDT itself only ever grows. So for the product what we would do is have two sets and we'd read them and as it changed we would tip to read them hand we would produce an output and in addition to producing the cartesian product of the sets themselves we produce the cartesian products of the two metadata and remember, this is tolerant to all these network anomalieslies.
   And finally we have the idea of inflationary reads and what an inflationary read is, is it ensures so we won't go into the details, there's a good conversation, a good blog post by lintsy Cooper about the difference between inflations and monotonicity. But what it allows us to do is if we're reading from a particular replica and we see state S and we see state S prime we guarantee that we won't read a value from that replica until it's greater so if you're familiar with distributed databases, this is related to a session guarantee. If I go to another node, I might have to wait for those results to be available, but this ensures that we continue forward processing of these transformations under failure conditions in the network.
   So we'll look at a quick example of what these streams look like to just demonstrate, so if you imagine we have two clients here, and we have a replica, so they both start off with the empty set and we're using the same triple notation that we had before. If one is added with A in the add set and that's sent to replica A and concurrently C2 adds with B and is sent to replica A, it merges them together as it sees them. So this is an important distinction. So in this system we're always growing, we're always moving forward and C1 can continue to apply updates to its local site. In this example, this stream at replica A is monotonic, this is always growing over time. It's actually inflationary because a mutation is applied and the resulting state is always greater.
   So if we want to build a system to compose these, we do this with this idea of the monotonic process. So again, we'll go back to where we're using a similar example, replica A has an empty set. We have a process which is going to apply our map function and then F of RA we're just using that for shorthand. So we start by saying OK, I'm the process, I want to read against replica A and the last one I saw was bottom, so the empty set. When re replica A changes and one is added with the addition of A, this reads returns to process 1 and says yes, I have a state that's greater than your last observed state and P1 processes that value and then propagates that value to the other CRDTs. So this F of RA value is just the -- I guess that doesn't work.
   It will say OK I'm going to give you a strict read again and then that read is not going to return until it's greater again. Here we do this with state. You probably say, wow, it's really ininefficient to keep the whole state around.
   Finally the state changes again, 1 is removed -- but the element is still deleted so this map transformation is fairly trivial but you can imagine that something like a fold is a lot more complex and even a filter is a lot more complex because you can't eliminate from the filter. So with the filter you need to kind of transform elements from active to deleted. And if we have a node that sees this execution, the confluence property we get through this monotonicity is that somebody else also could see this execution, right? And they only see some subset of the states, but as long as they get that final message, as long as we guarantee that they both see some final message about some piece of state, we'll see that the result is still going to be the same. So this is really nice because we can reduce the times that these processes need to communicate with the server, and still have the computation be correct once we've delivered all the events in the system.
   So finally we'll briefly talk about some work that's in progress called selective hearing this is an epidemic broadcast based run time system that we've developed. So if you're familiar with gossip protocol we build on the gogs yip protocol, would. Allow for very efficient dissemination of information so reliable broadcast under failures. That can handle very large amount of nodes and evaluations. And what makes this matching so brilliant is that broadcast protocols achieve their efficiency through weak ordering so this is why it's not a general purpose model to build things on but that's great because we don't care about message ordering. Our entire computational model builds upon the fact that messages can be reordered and we guarantee that we converge correctly and finally the reason for the name selective hearing is that we want nodes to be able to selectively kind of opt into computation so if I have a program that requires some variable state and I know that that variable state is on these five nodes, then I can express interest in in that variable so it's kind of a run time that resembled a published subscribe system. So I've shown you that can eliminate accidental nondeterminism through CRDTss, we can find a way to compose these things and build other CRDTs and finally we can efficiently distribute them with a really nice run time that's efficient. So now the question is well, can we actually build anything with any of this? Hopefully. So we're going to talk about a mobile game platform, so we'll use Angry Birds as an example. Advertisements are going to be displayed within game. Clients are going to go offline and people are going to continue seeing adds in the game. so we want people to continue to make progress, still add the impressions. So this is what the data flow graph looks like and we'll walk through it. So let's imagine that we have Riot Games and Rovio and they want to -- so they have a bunch of add counters games and we want to group them into a set and so let's imagine that there's a notion of contracts to make the example a little more difficult and contracts are saying that an ad is I believe able to be displayed at a particular time so what you had a he want to do in Ruby on Rails or something similar to that is you would say to your SQL database that in the data flow, we can kind of computing a cartesian product and then doing the filter and then only getting the ads that are active. We can give these clients all out to so we can distribute these counters out to the clients, the clients can continue to increment these counters, so they have a counter that represents kind of the globing ebb global counter and then they divergence from that and then they periodically send their counters back to like the data center and then the data center runs a process that's doing one of those inflationary reads. It don't let this process continue running until the advertisement reaches 50,000 so all of these clients are continuously merging their state in, and then when we hit that trigger value we remove the models from the set, that flushes down to the clients, this is done completely the only part we have coordination is where we synchronize with the server to send our state. Completely monotonic, we can disable advertisements and contracts all through mono tonicity.
   The data structure is correctly capture the concurrency in the system and are guaranteed to merge correctly so we can arbitrarily converge this graph and that's really nice. And finally if we think of the system as having a global truth, if we had an outside observer who could see what the global truth of the counters are, we could measure divergence as how often we synchronize and put our results into the global result.
   So, I got to wrap up. I think I'm behind. So what do I like about this model? We have built up from 0 synchronization, we didn't build a system where we had to remove coordination. The tradeoff that we made was that if we want an advertisement counter to be more correct, to diverge less, we synchronize more, but the application design uses no coordination at the lowest level and this is the really nice way to build applications. So finally just a kind of talk about what's next briefly because I'm running out of time, metadata can grow very large, we've shown that. We have mechanisms for reducing the metadata in the system, woo he have efficient representations of sets that use vectors rather than storing all of the operations.
   And we actually have an unpublished appendix to our original paper that extends to to a bunch of other optimized data types that will hopefully make a public appearance soon. Our fold function is not amazingly general it relies on the binary function applied to fold to be associative, computative, item potent and the ability to be mutative. And I'm folding that into ha counter, I need to have a way to the reflection the increments of the counter and the decrements of the counter in a way where I conserve the metadata between the two objects completely and finally, what can we derive from the data flow graph? So can we do things where we see that hey, you're running this map on this node and this filter on this node, well, we can just combine these things, right, into a fold? So can we use the data flow graph to drive distribution and can we use a minimum distribution to derive an optimized data flow graph so this is stuff that I am really interesting in are but have not even begun to think about but it seems exciting so to kind of wrap up. What have we learned?
   We've learned that we want to eliminate accidental nondeterminism and we've learned that we can do this with CRDTs and this provides a really great building block to do compositions of CRDTs, to build programs that observe this strong convergence property. And once we do that, if we have an efficient way to distribute this over nodes that's resilient, right because you have to be tolerant up to some number of failures depending on how you replicate, can we build a run-time system that allows us to do very efficient scaling of this programming model? So to wrap up, what are the key points?
   Synchronization is expensive, you probably all know this. You know, you've never going to get faster than the critical section that you're locking on. This is kind of straightforward concurrent programming. A lot of these applications are not possible. It's not possible to synchronize always. So if you're a mobile application, clients are going to be offline occasionally. A funny anecdote was I said at one point, you know, call your friends, computer round of consensus, tell them to get online at 7 and finally, you know, some things require synchronization, global environments, atomic visibility. So we have a bunch of papers. You can check them out if you like. We've tagged a release 0.0.1. We had a lot of time to get the tagging down, even though I have extensive experience with it. You can run our demo problem. And finally I'd like to European Union ... not only did they fund my research over the last few years but they also gave me a fellowship. I'm Chris Meiklejohn, thank you for coming. So I don't see Alex but I'm pretty sure I'm out of time ... ...
   [break]
