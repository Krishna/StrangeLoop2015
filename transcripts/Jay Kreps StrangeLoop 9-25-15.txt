   Jay Kreps: "Apache Kafka and the Next 700 Stream Processing Systems"
   
   (Live captioning by White Coat Captioning @whitecoatcapxg) 
   
   
   >> All right, I'm going to get started.
   >> So thanks everyone for coming. This talk is going to be about Apache Kafka and the next 700 stream processing systems. I don't know if it will be 700 exactly, but hopefully around that. My name is Jay. This is an area I've been working in for roughly the last five years, I'm one of the original authors of Kafka, and so now I'll try and condense everything I know about this area into, you know, 35 minutes.
   So the name comes from this old 1960s computer programming paper on a system called I swim and what I liked about this paper was it tried to break down this language into kind of constituent parts, and see how you could recombine those parts to get different things, so that's a lot of what we've tried to do with Kafka and I'll try and explain in this talk some of the parts that make up Kafka, and how they can get recombined tore stream processing.
   But before I do that, I should probably mention you know, what stream processing is. How many people have heard the phrase? Probably some. Most? Everybody. OK, so everybody already knows, you can actually skip this section of the talk, then.
   So my definition of stream processing is going to be a little different. I happen to think this is one of the most exciting things going on in technology right now. I' a little biased because I'm working in the area but I think it's really exciting. The way I think of it is there's really three paradigms for computer processing. Of course there's lots of ways to slice up programming by lots of different languages and whatever, but one way to slice up the pie is how you get your input and how you produce your output. So the systems that most of us work are request systems. People are also familiar with batch computation and stream stream processing is actually a third thing. So I'll talk about much ooh of these briefly.
   So request response is really HTTP, arrest service, pretty much most of the things people make, you know, somebody send you a request which is really one chunk of data, one input and you send back one output, one response and of course there's lots of these requests and this is really the way most systems are built and usually this is synchronous, like when I send up the request I'm expecting the response. But mostly most things are batch processes that kind of turn over once a day, sometimes once a week and these are different. They produce all the outputs all at once. I like this picture, I don't know what the guy is doing, I think he's listening to see if his Hadoop job is finishing and she's like it never finishes when you're listening like that. It's surprising that we have all this batch stuff around. But not all that surprising. Probably most people's first computer program was a batch program, right? There was actually some advantages to this type of computation. It tends to be really efficient, or at least it can be because you're aallowed to preorganize the data. Often really efficient. In fact if you ever want to do something really scary, take all the input requests for some sort of service oriented architect and rewrite your logic like a batch Python script or something and process them all in batch, you'll actually find it runs about a thousand times faster why? Because you can reorganize all the data into the right spots if you do it that way and this why this stuff still exists and between these two, request response and batch, that's pretty much everything. But there is an alternative which is getting more popular and is out there which is stream processing and it's been at the periphery. There are certain systems that work this way and just now starting to become mainstream and the difference in stream processing as opposed to these other two is instead of getting one input and producing one output or getting all the inputs and producing all the outputs, now the program actually has the control. So the program kind of gets some inputs and produces some outpulse and how much is some? Well that's up to the program. It can also do everything in between. So it's kind of a generalization of these two extremes. And what's exciting about this is like a request response, like client server, it runs forever, but it also gives you kind of total control over the tradeoff between latency and efficiency. And it allows you to do a lot of the kind of complicated analytical things you might have in these big kind of batch jobs but it allows you to do them really quick.
   And you know, what it doesn't mean, what I just gave is my definition. A lot of people will have a different definition in mind. And what it doesn't mean is computation which is, you know, transient or approximate or lossy and a lot of systems that have done stream processing have been that way, so, you know, for a while you would see stream processing systems and kind of the way they worked is you would throw data at them and they would try and compute something and they might get the right answer and if things came too fast maybe everything would fall on the floor but there's nothing inherent about that. That was just a weakness of the system. You can absolutely make stream processing get the exact right answer as you would expect with the batch process and it can compute, you know, the full set of things that are computable.
   >> The domain of stream processing usually asynchronous stuff. Like I said it kind generalizes. This kind of decoupled work is really useful, you know, this is where all the kind of back end intelligence of a lot of companies happen and in general I found when building systems whatever you can kind of take out of the synchronous part of a request and put into something asynchronous is usually better, it's usually more efficient, usually safer when it fails because it doesn't take everything else with it .
   And a lot of people may have seen some of these kind of stream processing programming things, this is some reactive X snippet I took of 0 the web and so you'll see a lot of these APIs designed around stream processing and getting that done but what I'm really talking about is not so much this. This is you know, like the kind of RX java or these other things, these are libraries for doing computation in one process and they present really a user interface to processing a stream of data and that's very useful but what I actually am more interested in is called stream processing in the large. And this is a trend that you'll see in a lot of places. The scope that computer programmers are using has now expanded. So no longer focused on the internals of a program. They're actually kind of zoomed out and they're focused on computer programming at the data center level. So a good example of that would be something like Mesos, it kind of starts and stops processes. And the same way I'm actually interested in stream processing in this way. So being able to represent what's happening inside of a company, as a set of streams, and it turns out I think this is a pretty good metaphor for what a company or organization or a large data system does, right? It has streams of inputs that represent all the kind of new things coming into a company, it has a set of processes that run that respond to those inputs. It has some state about what it currently knows and it has outputs, hopefully one of those outputs is money, but not always. Right? .
   So if you think about something about retail, I think everybody is familiar with retail stores, you have sales, you can represent kind of a stream of continuous sales. You have a stream of shipments, you have a stream of maybe price adjustments, and inventory adjustments, you have analystics and fraud and you have to reorder products and you can actually map this pretty well to streams and stream processing, these price adjustment processes and inventory processes, analytic processes are probably stream processing that react to this and do something. Now they may not be implemented that way. Probably in real source batch processers, but they could be implemented and it's much faster and kind of a better metaphor for what's happening.
   But the kind of problem and the reason you haven't seen as much stream processing stuff to date is really there hasn't been the infrastructure for it. You know, the world of kind of request response things that's up there at the top, this is your OLTP databases, you know, Rust frameworks, it's actually pretty well developed and it targets getting a response usually in a few milliseconds. There's pretty good infrastructure there and if you come down to the bottom you've got batch stuff, like Hadoop,terra data, there's a lot of supporting stuff if you want to do processing there. But in the middle there hasn't been that much to help people out, right and if you want to build anything that's slower than a few milliseconds but faster than a now hours, you kind of are on your own, you're kind of inventing it from scratch. There has been some stuff, enterprise messaging systems, CEP. There's thing called an enterprise service bus that hopefully not that many people have heard about. Database triggers. But none of these are really good. None of these are really technologies you would want to develop a large meaty peace of company infrastructure top of and the work that's done in this domain tends to get pushed upwards into or downwards and there's kind of a vacuum in the middle. That's really what we've been trying to address.
   And the reason there hasn't been -- the reason that there hasn't be been much here is there's a number of hard problems that a stream processing system has to address. You know, the first is since I said I was talking about doing this in the large is partitioning and scalability. How do you spread a program over many machines and be able to elastically add capacity to that or shrink it down and how do you spread the data, as well? Semantics and fault tolerance, what does it means R. mean when one of these machines fails? What do you? There's this whole problem of unifying streams of data with tables of data. So in the retail example there were certain things that were really tables like what's our stock on hand. And certain things that are really sales and what's occurring and how do you put those two things together gracefully and finally time. It's particularly bad in distributive systems where there isn't really, the notions of time become many and stream processing suffers from this even more. In the request response time you ignore time so you query lots of services they're all kind of at now. What does now mean? I don't know but it's roughly now, right?
   In the batch world they actually just control time by loading all the data at the beginning of the day and then not changing anything until the end of the day when they do the load again, right? So stream processing is going to have a much harder time because it has to continuously, you know, account for change, but it may actually need to catch up with older data and finally reprocessing, so let's say I have a stream processing system and I maybe count things that are occurring, if I change the logic in my program I'm going to want to rerun that program and get new answers again.
   So OK, so that was kind of an introduction to the area. The now comes Apache Kafka. I started with my coworkers when we were all at Lincoln. Stream database or something, and you have lots of producers that distribute messages into Kafka, it kind of maintains all these streams of data, it maintains them in a fault tolerant so that each piece of data is stored on multiple machines and it can handle failure and stuff like that and consumers can tap to the stream. And how it does it internally what Kafka, you know, stores is a log and not everybody has seen this idea of a log. It's actually a very simple idea. These little rectangles are meant to represent, you know, messages or records, and each record I've kind of given a number, like 1, 2, 3, 4, 5. And different readers could be reading, they always read from left to right and so you could think of this as being kind of like a formalization of the log files you get out of your applications, so like an Apache log, right? But this is a little bit crisper. It may actually be binary data. I don't know what the context of each of these records are. I've given each line a formal number and I'm actually allowing readers to kind of subscribe to it so you could think of it more like a commit log in a database. But it's not very different from an apache log. And it turns out that this dataa structure is very closely related to the problem of consensus or having many distributed things agree on an answer, so most people would use a log as an implementation of something like raft or multi-PAXo, so these are kind of maintaining a log. And they show up in databases, so inside of a distributed database, you will often find some type of log of changes, it's the core set of what data was modified, and not all databases make the log part explicit. But certainly many of them do, and I know one of the big distributed databases at yahoo does this, the database at LinkedIn where I was did this. The know the big database at Twitter does this so it's not uncommon to have a explicit log system that is recording these changes and since this is pa talk about stream processing, I would argue this type of log is actually the kind of physical manifestation of a stream. So I talked about streams of data. What does that mean? Well, I think formally defined it's going to be a sequence of records. And the only difference I would add is you're probably going to partition this up into multiple logs so you have some notion of parallelism, right? If everything is totally ordered everybody has to coordinate to of maintain that order and since many things happen in parallel in a large company you're going to have many of these partitioned logs. So producers are going to add information, consumers are going to read them. Kafka maintains logs and tries to do it at large scale efficiently. Now we know what a stream is. Stream processing, you know, in my world view is really just transforming some of these input logs into output logs. We said log was basically like a stream so the processer is P going to be your code in some sense and there can be a framework which you run your code in which helps make it easier to write the program but it could also be just a Python program. There's no magic, if you're transforming one stream into another stream you're doing stream processing and you can pat yourself on the back. One thing I've kind of called out here is there's a little database-looking thing inside the block of your code and that is state. That, you know, stream processing, the easy problem is if you give me one input and I just give you an output then it's not very hard but if I have to maintain some kind of state, that is you know, a count or a join or something that is going to span some length of time, it could be the whole execution of the job or it could be counts over a five-minute window then I'm going to have this state with my job and I have to make sure that that state is kind of protected even if my code dies and that's going to be one of the hard problems.
   OK. So you know, an important thing to understand in this area is the concept of a change log. So I talked about logs and I talked about streams, but one of the core uses, especially in data systems is maintaining a log of changes so here I've drawn out a series of put operations so mutations, right, updates. And you have a bunch of modifications of the same small set of keys and you can use this to represent the notion of change over time so the progression I have log from left to right that's basically time and these put operations that is are recorded in the log this is the set of changes which represent the database. So anybody who's interested in functional programming and knows about consistent data structures this is the exact same thing but applied at a larger scale.
   Like oracle has kind of a log shipping protocol and S mySQL has a log shipping protocol. Kafka happens to have a particular facility for maintaining this type of mutations to state which is what we call log compaction, which allows you to take these type of redundant updates and compact them down by getting rid of the redundant ones over time and this is important because it's going to turn into a way of maintaining that persistent state that we've talked about. We've gone through logs, partitioned up, fault tolerance, the final thing that we have to talk about is how to actually scale the stuff outside of Kafka, the consumers of data. Is so it's not enough to be able to horizontally scale the data in Kafka, you have to be able to scale the processing of data, otherwise there's really no point and the facility that Kafka has for this is called groups so it allows many processes to all kind of join a group and know who are in that group and so originally we did this with Zookeeper, and what this allows this group of consumers to do is divide up all the logs and process its own subset of logs and it does this dynamically so you can add new consumers and they'll join pa group and if some of them die they'll come out of the group and you know their work is given to other people and this is maintained dynamically and there's multiple groups because Kafka allows multiple readers for the stream so all these streams are multireader.
   So those are kind of the ingredients that Kafka provides. How do these come together and attack some of these. I'm going to give a quick outline of some of the ways that that happens.
   The actual mechanisms for doing stream processing with Kafka, the most popular one is just in your code I guess people consume data and do stuff. But there's a bunch of frameworks which have emerged and do this. Spark has a streaming, storm is a framework which works well with Kafka. SAM St. A, there's another system, FLINK. And the thing I'm going to focus on most is new work that will just be coming out in the next month which is called Kafka streams and it is not a whole framework or distributed but it does the same things that most of these systems do so it will kind of address these harder problems that I've talked about.
   OK, so the first hard problem was partitioning and scalability. In Kafka this is done by basically dividing up these logs and with this group management feature. Kafka streams or these other stream processing systems can divide up the work. What makes this hard is the fact that these processes are allow to marital relationship taken state so if you were here for the last time, I think it was on stateful services in some ways each processer is its own stateful service and it has to solve those problems with what happens if I end up somewhere else and I don't have my state. And the way we do this in Kafka is using this change log feature, so Kafka allows you to make change logs like logs of updates so one of these processers which is keeping local state, it can keep it in memory, it can actually keep it however it wants, it journals out these changes to Kafka and that acts as fault tolerant backup of everything that's happening and the new instance of that processer can always restore off that if it needs to and this allows Kafka to representing the initiation of a stream and the initiation of a table. And put these two things together is actually really important because what people want to do with streams of data is usually join them onto existing streams of data and unifying this notion of a log of changes with an instantiation of those changes like a key value store is one of the key things that these stream processing have systems have to address.
   Fault tolerance there's a bunch of aspects of this and it's too much to get into in a talk like this. Kafka allows you to restore your kind of state off these change logs and since the data streams themselves are repliable, whatever hasn't quite been processed yet can be pulled back.  There's more work to do here in Kafka, there's actually a prototype of a more complete kind of atomic transaction that will strengthen this but a lot of these systems that do stream processing rely on that change mechanism to be able to provide their guarantees.
   The final thing is time. We didn't fully understand this in the beginning. But it turns out you really have to be able to deal with late arriving data. It turns out that really the solution of that time problem is maintaining counts as sort of mutable tables that can keep being updated even after that time has passed on the local clock. So those were some of the hard problems. It turns out if you solve these problems you actually do kind of achieve this unification of batch processing and stream processing, and the way you do that is pretty easy so if you had this log abstraction, a batch process is one which kind of wakes up, it has some position in the log, maybe it's time 3 here, it processes forward until it's time, you know, it reaches the end. Then it goes back to sleep. Maybe it wakes up again at the next day and it props more. And then it goes back to sleep and it wakes up again. So batch processing is actually reallyism simple. It goes to the end and thus is self down. Stream processing stays alive. So this log processing helps you unify batch processing which happens on some kind of regular schedule which stream processing which happens continuously as data aarrives. And that kind of brings us to this problem what do you do when you change your code and you need to recompute things. It's really simple you go back to the beginning of time and you reprocess. It may not be everything that has happened because you may have compacted that log to the most recent updates. And this notion of time in Kafka is called the offset. That's the position in log and 0 is kind of the very beginning so any time you change the code you can reprocess in this stream processing system by just rewinding to the beginning and letting it rewind and this can happen in parallel because these topics are actually multi-subscriber.
   OK, so how does this actually play out in the large? We started to put this stuff into practice hand we calmed the idea a stream data platform, so up at the top here I have those kind of request response systems, those are applications, arrest systems, database and you can channel these into Kafka and have the set of streams of what's happening in the company. You can attach to this stream processers which do transformation on the straps streams and they transform them into new streams and publish them back and feeding off that in this asynchronous domain is analystics, a lot of the kind of intelligence stuff that's happening quickly. The middle layer is your asynchronous stream processing area and finally you can feed these same streams into Hadoop or building warehouse. so Kafka is in thousands of companies now, and at linked in it was actually, I think as of a few weeks ago is actually taking about 1.1 trillion messages or records per day that flow through this kind of central message broken or persisted and that's the feed of all the data into the offline world. It's the basis for all the stream processing or real time Analytics kind of stuff and it's kind of like one of the core data systems for storage and other data systems that rely on this as a kind of commit log.
   And so if you're interested in these ideas, Kafka is an open source project it's an an Apache project, you can read about it there. We have a pretty active blog about Kafka and stream processing stuff at confluent. There's actually a design document if you Google KIP-28 where you can read about it and I have stickers if you want them. I'm pretty much out of time. If you have questions or you want stickers, just come around and find me, I'm the really tall guy. Thanks so much.
   [applause]
