   Martin Kleppmann: "Transactions: Myths, Surprises and Opportunities"
   
   (Live captioning by Norma Miller @whitecoatcapxg)
   
   >> Hello, everybody. Thank you very much for coming. My name is Martin Kleppmann, I eventually settled on the title Transactions: Myths, Surprises and Opportunities, because, you know, it at least had a bit of a feeling of optimism in there. My actual working title for this was more like transactions: Joys, challenges and misery, with maybe a subtitle of consistency guarantees? You want consistency guaranties? Hahaha haha, --
   >> So you see looking at this topic has driven me a little bit insane and it's possible that the forbidden knowledge in this talk will drive you insane, too, so this is a little experiment to see if I can drive a few hundred people insane within the space of about 40 minutes. So this talk came out of some research I did for this book that I've been writing for ages and which still isn't finished, but this book called dataintensive.net, if you're see interested. Anyway when we're talking about transactions I should really briefly go back into the history where they came from and origin is back in the mid 70s, and this really laid the ground for a lot of subsequent relational database systems and if you look at the style of what all the common relational databases now do, a lot of the ideas there can be traced fairly directly back to System R, actually and the idea of transactions that you've got in System R was then kind of kept for the next I don't know, 30 years or so, until around the late 2000s or so, this new SQL thing started becoming a big movement and then of course the inevitable counter-movement came but really these things are less about SQL and more about transactions I have the theory. But really the transactional guarantees are what most differentiates these different technologies. And of course if we're talking about transactions we're talking about acid, which has been mentioned many times, but I do actually want to unpack what this means in a little bit more detail , because we kind of generally think we know what it means, we think we know it means ought mowticity consistency, isolation and durability but what do those words actually mean and if you look at it in more detail, you find out that they're a bit vague. Acid as a term is more mnemonic than precise which is a polite way of saying that it doesn't really mean anything. It's just kind of this marketing term that we use. But nevertheless there are a certain amount of definitions the let's start with durability which is kind of the easiest. Back in the day it meant you wrote your transaction to an archive tape and because you know, discs were not that reliable and you you would simply aappend all the transaction logs sequentially to the tape and if your discs died you could then replay the tape and but then of course tapes fell out of fashion, we got disks instead and people started defining durability of meaning f synch to disk. But that's really all I'll say about durability, otherwise there's nothing too interesting there.
   What about the C, the consistency bit? Now, it should hopefully be clear that the C in acid is absolutely not the same as the C in the CAP theorem. If somebody comes up to you and says we can't have acid because CAP theorem says so then I suggest you very quickly run in the opposite direction because you're probably not going to have a very productive conversationment so I wrote this paper about CAP theorem in analyzing it in a bit more detail if you're interested in it. We discussed it last night. I'm not going to go into detail in it today. So the C in acid is kind of an odd one in that it doesn't really have the same kind of structural property as the A, the I, and the D do. Joe Hellerstein suggested that it was kind of basically tossed in by the authors to I can ma the acronym work, but they didn't really attach much importance to it. So C is now generally associated as the database, the transactions, the application executes on the database, move the database from one consistent state to another, where consistent state here is defined through it meets certain invariants and those invariants could be expressed as integrity constraints that are checked by the database, but they might be implicit, as well, so like in an accounting system you might have the invariants that all of the credits have to sum up to all of the debits or something like that, and in some systems you can express that kind of invariant declare tively and others not, but generally the odd thing about C is it's really a property of how the application uses the database, it's not a property of the database itself. So that's why we won't really go into consistent in too much detail either.
   What about the A? About atomicity. This is kind of a confusing one. Because in the context of multi-threaded programming, you have things like an atomic increment and that means that you can have multiple threads incrementing some value, and you're not going to have lost updates due to reading delayed writing, but in the context of acid, A, the atomicity is not about concurrency, confusingly. Concurrency is dealt with I. So here in the context of acid, atomicity is about how faults are handled. If things crash or things go wrong, what atomicity provides is that you're going to get either all or none of the writes that happen during a transaction, so you can think about this for example, like this, say some timeline here, time moves from left to right. So you've got an auction website and somebody has just won an auction, and so you want to do several things now with the winner of that auction, you want to update the listing to say oh, the buyer is the user who bought it and you probably want to send the buyer an invoice and maybe some other things, as well, and now those are several writes that have to go to different places and what happens if something crashes halfway through making those various changes? You could now end up in some inconsistent state where some changes have been made and others not. And the whole point of atomicity is that if you wrap these things in a transaction, you can say, OK, the incomplete changes due to the aborted transaction will be rolled back and won't be seen. So fundamentally here this means transactions are about manipulating multiple objects as one, but doing that as one unit. So if somebody has transactions, claims they have transactions, but actually it's just operations on a single object, like a single document in a document database, for example, that's not really a transaction, because the whole point is that you can wrap several different mutations in one logical unit and if something fails, roll back any partial modifications that you made. So if I was reinterpreting the terminology, rather than atomicity, I would actually call this abortability, because that's really the key feature of a transaction is there's always the possibility that it can abort and there's well defined semantics of what happens when it boars, namely all of the writes that it made, all of the side effects are null and void and rolled back. So the nice things about aborts is that they allow you to collapse a whole range of different failure classes down into one thing that you have to handle. So it doesn't matter whether the problem is a deadlock due to concurrent transactions locks or a network problem, or maybe some integrity constraint is violated or maybe some an application server crashed or maybe a database server crashed or somebody yanked out a power cable. Doesn't matter. All of these things all just collapse down to a single thing that you have to handle which is an abort and you could for example retry that transaction. So that's all about atomicity. So let's move on to the isolation which is kind of the most tricky of all of these which is why I've left it to last.
   If you read the database textbook, it will say that when you mean acids, isolation, you mean serializable isolation, which is a rather strong definition of isolation. It means that the effect of all of the transactions is as though they had executed serially, one after another, on the database. That is, each transaction feels as though it has the entire database to itself. Even though actually other things might be executing concurrently, but those concurrent things are never visible to the transaction.
   But in practice, there is serializable isolation, because there's also this whole raft of different isolation levels and the history of these can actually be traced directly back to System R where, you know, they first implemented serializable isolation and they realized it's kind of slow, so what do we do? Let's fiddle with our lock implementation a little bit and let's just change some of the locks so that they're not held so long, and then let's give those levels with different lock configurations names and that's where these names come from, but really the definition of these is terms of a 1970s, implementation details and that's why these things are so hard to understand because they're not defined as something that should make intuitive sense, they're just defined as implementation details of a particular database. So is there anyone here who thinks they can explain off the top of their heads precisely what is the difference between read committed or repeatable read. Anyone here? If so if the assembled cannot tell the difference between all of these things even though I'll assume that all of you have used a relational database at some point so in theory you should be able to make an educated choice but in practice you don't understand so how are you supposed to make a choice? So let me look at this in a bit more detail. Here's a wonderful list that Peter Bailis and his folks at Berkeley made looking at what is the maximum isolation levels and what is the default if you don't change the default and if you look at it the maximum level is serializable on only about half of them. So half of them don't support serializability is only the default in 3 out of 20 or so. And this means that you'd have a lot of this complexity to actually think about if you want to write an application using one of these databases that doesn't have race conditions.
   Let me go into the differences between these in a bit more detail, because I think it's actually quite useful to look at some examples of race conditions that can occur. At these different isolation levels, and thus we start then unpacking a bit more what we mean by isolation. So I'll start with read committed which is actually a fairly easy one to understand and it kind of means mostly what you think it should mean.
   Sorry, skipped over a slide there. It's recommitted means that two phenomena, two anomalies not allowed to ever occur. Dirty reads simply means that one reads data that another transaction has written but not yet Whitted. So that's what read committed means. Dirty writes is maybe not y quite as obvious. Imagine you have, this could be again the auction website example, for example, so you've got two transaction, A and B, and they are writing to two different keys and key value store or two different rows in a relation database, whatever. And they're called X and Y. And they concurrently write to both of these, and the idea is that you end up with either X being A and Y being A or you end up with X being B and Y being B. Those are the two things that could occur. But in this execution you actually end up with X is set to A and the final value is B with areas Y is first set to B and the final value is B and the two are now conconsistent. But read committed doesn't permit this. But we have all of these other things, so what do they actually mean there? There must be some other kinds of race condition that can occur at read committed which these higher isolation levels prevent. Let's look at this one. It's called read skew. This could be an accounting system or a bank or so and you've got two accounts called X and Y and they have a starting balance of $500 and now you've got a transaches that wants to transfer $100 from one account to the other. You decrement one account by 100 and increment the other by 100. And this is fine. Now consider that you have a read only transaction, it's only reading the database, not changing anything and maybe it's a backup process or an Analytics query or something like that, which needs to look at lots of different keys in the database and at some point it reads Y and it gets 500 back and at some later point it reads X and gets 400 back and it's valid for it to get 400 back, because at the time it's reading X that transaction that's doing the transaction is already committed so it's reading committed data, which is fine but through we've got this inconsistency which originally there's a thousand dollars in the total in system but it looks like there's only 900 in the system so it looks like 100 has vanished into mid air. If you restore from this backup, your database is going to be meaningless because you've seen different parts of the database at different points in time. And this can happen under read committed and like read committed is the default isolation level for quite a lot of those databases here and in order to prevend this read skew anomaly here, repeatable read and snapshot were isolation were brought in. They're fairly similar looking, implementationwise they're quite different. In many cases now when databases say repeatable read they're basically snapshot isolation, which can be confusing.
   This is what SQL server does for example, and this is what System R did back in the day but more common nowadays is the snapshot isolation which is implemented as multi-version concurrency control, so this is what Postgres does and mySQL does. The purpose of snapshot isolation is if you're a transaction that is reading the database, you see the entire database as it was as of one point in time. So even if other things -- other writes are subsequently committed that read transaction, even if it runs for a while is not going to see those, and that's where the multi-version comes in here, that means the database internally keeps several different versions for a data item and it will present each transaction with the one that's appropriate for like its time cut here.
   OK, but we're still not at the top. There's still a difference here between snapshot isolation and serializable. So can we find another example to distinguish these? What do we need to do? Now, imagine you have a -- say an emergency dispatch service, so like an ambulance service, for example. And the service needs to ensure the invariant that always at least one doctor is on call, because if no doctor is on call, you no, no ambulances can go. So from can be multiple doctors on call, that's OK. And doctors can trade their shifts, so if a doctor is not feeling well, they can ask a colleague to take over for them, that's no problem. And now this is the transaction that will get executed if a doctor wants to go off call, so they say I'm not feeling well, I click a button on the web interface saying can someone else take my shift, please? And this this transaction will first check how many doctors are currently on call and if this number is greater than or equal to 2, then it's safe for that doctor to go off call, so then the database can be updated and say the doctor is no longer on you will ca. It's a very simplified example, about you it's enough to show a particular anomaly showing. Say there are three doctors, Alice, Bob and Carol. And Alice says I'm not feeling well, can I go off call please and the transaction says yeah, we have two doctors currently on call it's safe for you to go off call. And then concurrently Bob does the same thing, he's also not feeling well but his transactions also sees two doctors on call and now both go through, both meet their condition and now we've got a database in which no doctors are on call and our invariant has been violated. But this problem is called write skew and it really does happen. I made a little test case that shows database under which isolation levels, it happens. And for example in Oracle you simply cannot prevent this, unless you build in specific locks or materializing the conflict in some way where you really have to think about what you're doing there. So just hope that your ambulance system in your town is not build on Oracle, I guess.
   But the general principle of this write skew, it can happen in many different situations, the general principle is that you have a transaction which reads something, and then makes some decision maced on what it has just read, so in this case the decision is are there enough doctors on call and it writes its decision to the database, but different transactions may write their decision to different places in the database and so they're not going to conflict, so just a simple lock is not going to pick up the fact that those transactions conflict. And the problem here is that with concurrent execution, by the time a transaction commits, the promise of the decision that this transaction made, that premise may no longer be true, and this would then be a serializability violation.
   OK, so we've worked our way up to serializable and we've realized that there are some fairly subtle but important race conditions that did happen at these various isolation levels. So how can we actually implement serializability well, for 30 years, or so the only real answer was use lots and lots and lots of locking and this technique is called 2 phase locking I'll abbreviate it here with 2PL and the way that generally works is this: Whenever you read something you need to then take a shared lock on all of the stuff that you've read, and the shared lock doesn't stop other transactions from reading it, but it does stop other transactions from writing it, so you can then be sure that once you've got that lock and you hold that lock until the end of the transaction, then nobody's going to come and modify those things that you've read, and the important thing here is really that you do have to hold the lock until the end of the transaction, which leads to some serious performance problems. Say you've got a transaction that needs to read all of the rows in the database, like a big Analytics query, for example, would scan everything. Well, that needs to now take a shared lock on the entire database, which means that nobody can write to the database until that transaction is completed. You're basically looking the entire database for write. And this is why all of these weak isolation levels came into existence, because people said there's no way I'm going to lock my entire database for writes, that's just not going to happen. Fortunately better techniques were developed so in the late 2000s, kind of two competing approaches came up as to how we could implement serializability without two-phase locking and I'll run through each of them briefly so the first example is from H-store which was an academic prototype but then became volt DB, the second example is from Postgres. So the first example the second is you can take all the locks and implement them in order. But construction there's no concurrency going on, so you simply take what would normally be these various interleaved threads of execution and turn them into a timeline. Of course this is only going to work if you make each transaction really, really fast to execute. And this is why it took 30 years to get to the point of, you know, of course theoretically people would have been able to do this for ages, but if you want to actually execute them in serial order, each transaction has to wait for all of the preceding ones, so a few things you need to do there is you probably need to have all of the data in memory, because waiting for disk to come back is just going to be too slow if you want to execute serially, and you probably also need to take the entire transaction and ship it to the database as a stored procedure, where the database can then just quickly execute it as soon as it's its turn. You're not going to have an interactive client server slowly back and forth over the network conversation, because that would just take far too long. But like this, if you can get your transaction executed within, I don't know, 100ms, then you can actually get some reasonable through-put and so this is the principle that's used in developed DB. Datomic uses similar transaction commit principle as well, I think.
   So that's one way of doing it. The third way which I find quite interesting is you can detect any serialization conflicts and abort transactions, and this is this technique is called serializable snapshot isolation, so it's based on snapshot isolation which we saw earlier but adds additional checks to it in order to make it serializable and this is what is used in Postgres since version 9.1. And in the first instance it actually looks a bit similar to two-phase locking. That is, you have locks that you take whenever you read something and whenever you write something. But the difference is that those locks don't block, so if somebody wants to write to something that has already been read and there's a read lock there, that write is not locked, it can still go through, these locks just record everything, take a little ledger and write down everything, OK, this write from this transaction appears to conflict with this read from this other transaction and just records all of those facts and then at the end of a transaction, when it's time to commit an analysis process happens and it looks at the various conflicts that occurred, and decides, is it safe for this transaction to commit and if there's the possibility that this transaction would introduce a serializability conflict. You're assuming that the conflict rate is not going to be too high because if you've got a lot of contention then you are you're going to have a lot of transaction abort and restarts, but if contention is not too high, then this can work very well. In contrast, two-phase locking is the pessimistic approach so it makes sure that you can never ever get into a situation where transactions observe nonserializable outcomes but at the price of preventing a lot of concurrency.
   OK, so that's our over view of these different serialization levels. There's still more to say, I think, kind of a big elephant in the room I feel with all of these transaction levels, and that is that all of them work within a single database and generally that database is assumed to be in one geographic location, but we're building bigger systems than this now. What about if you're doing micro-services and you've got several different services each with their own databases? What if you're doing stream processing and so you've got these flows which take in input streams, produce output streams?
   >> What do transactions mean in this kind of world? So we've got here these various kind of boxes, and there's a cylinder inside each box and the idea generally what I've heard recommended with micro-services is that you don't try to share a database between several different services. Because that would break a lot of the goals that of introducing micro-services in the first place, so you've got these wrappers of application code around each database, and if you're doing stream processing, well, the picture actually looks very similar, the difference is only with micro-services, these arrows here are request response like RPC or rest type data flows, and generally what people do, what seems to be the recommended thing, is to draw a box around each of these services and say this is a transaction boundary. Transactions only occur within one of these and if it's using a transactional data store, sure, it can use whatever facilities that data store provides, about you there's not going to be any transactions across different services.
   And there's a very good reason for that recommendation. And that is that if you want -- if you wanted serializable transactions across multiple services, you would have to run some kind of atomic commit protocol. So these are distributed systems protocols which make sure that if you've got various different parties who are all participating in the transaction, various different services, either all of them commit or none of them commit.
   >> And protocols such as two-phase commit, three-phase commits, transaction managers, etc., they implement this kind of thing. However, has we saw earlier, the whole idea of serializability is that transactions appear as if they were in a total serial order. Now, you can can execute literally in serial order or not, but that order is definitely defined, so whenever two transactions conflict, somebody has to decide which of the two came first.
   And this from a hey, hey, stop stop stop -- from a distributed systems point of view, this makes it effectively an atomic broadcast problem, that is, the problem of getting a bunch of messages to different nodes in exactly the same order. but the issue interest with atomic broadcast is it's actually equivalent to consensus. And consensus, we can definitely do, but it's pretty expensive to do. It requires a lot of coordination between different services, because we've got to get them to agree on something. And people tend to not like doing things like two-phase commit in services because it makes them very brittle, very sensitive to failures, because it only takes one service which is running slow or which is temporarily not responding and then suddenly everyone else can't get any work done, either, so it tends to amplify failures. This coordination makes systems very brittle whereas the whole point of wanting to introduce several services was to make them more decoupled from each other so you could have independent failure domains. The whole thing even gets more fun if they're distributed geographically because if you're trying to do consensus over a very large network with a lat of latencies, a that's why people recommend run Zookeeper only in one data center, for example. Don't try to run it across different data centers. So we seem to be stuck in this place where there's no transactions across different services. But what do people do instead?
   >> So what I've seen recommended, and the context of micro-services is well, you might have compensating transactions, for example. That is, you try to do something, and the service does it, and then you do something somewhere else, and that fails, and now we have to say back to the first service, oh, actually, could you undo that again, please? Because this other thing that was also supposed to happen didn't happen and so in order to get back into a consistent state, you have to do some cleanup afterwards, which looks somewhat like transaction abort and roll-back except that you're implementing it at the application level rather than it being implemented at the database in the infrastructure. There's a paper describing this kind of stuff.
   What do people do about integrity constraints in this world? So say you have -- I don't know, a constraint like you can only ship as many items as you have in the warehouse, what happens if you accidentally accept more purchases than you have items in the warehouse? Well, you're going to have to apologize to customers and say oh, sorry, actually we ran out, here's a discount code for your next purchase and we've refunded you or something like that and this is reasonable, like businesses do this all the time. but you can think of these kind of apologies is really you have some integrity constraints but you don't enforce it up front, instead, you check after the fact whether the integrity constraints was violated and if it was violated you fix it up again so bringing this back to the acid terminology it seems like this transactions are bit like the atomicity. And doing apologies,, so this kind of heads me to thinking, you're going to hate me for this. This leads me to thinking every sufficiently complex and large deployment of micro-services contains an ad hoc informally specified bug-ridden slow implementation of half of transactions. I have no evidence whatsoever to back this up. I'm just asserting it boldly.
   So what about the I in acid? So I said like compensating transactions are kind of like implementing atomicity and apologies are kind of like implementing consistency. What about isolation?
   So take another example, say you've implemented a social network, and I don't know, one of your users has just broken up with their significant other and they're really unhappy and so they go to your website and they unfriend their previous -- their ex-girlfriend or boyfriend and now after that, they send a message to all of their friends saying how horrible this person is and the intention of the user here is clearly that their ex boyfriend or girlfriend does not receive that message because they first did the unfriending and then they posted the message. So like if I was a user, this would make sentence to me in my mental model of the world. Except now if you've got the friend's database handled by one micro-service and the posting database handled by another micro-service, this ordering information between the two -- these two events, is lost. So you could have for example notification service which listens to these, and due to some transient glitch in the network or so, the unfriend event arrives later at that notification service than the post event, so by the time the unfriend event arrives there, it's already sent notifications to all of the people including the ex boyfriend or girlfriend who was not supposed to get the notification. So problem here is that there's a causal relationship. The user hasn't told us that this posting of message to friend depends on the unfriending event, it's just implicit through their timeline, about you in this micro-services world we've somehow lost that information. Now, this leads us to some interesting area of speculation. And/or research which is a similar thing, where at the moment, the way we're building micro-service-based systems is just pure eventual consistency. Hopefully it's at least eventually consistent. Maybe it might be perpetually inconsistent, who knows, about you achieve consensency eventually. So we want something in between which will still capture causality, which could still capture this kind of ordering of events and allow people to reason more in a logical way about this, and causality is quite a nice thing, actually because if you think about what's going on in snapshot isolation, you know, where I said you get a consistent snapshot or at one point in time, that the transaction can read from. Well, what does a consistent snapshot actually mean? It means that it obeys causality, that is, that if you see one item in the database, and that was written after some other item, then that other item must also be in the database. That's really what a consistent snapshot here means and the interesting thing is this can all be implemented without requiring global coordination, without doing consensus. There are nice proofs which show that this is actually the upper bounds of consistency that can be achieved. Whether we can do it consistently is another question. There are other proofs that show you get quite a lot of metadata overhead if you want to track causality but I think that's going to be an interesting fruitful area of research, can we make causality efficient? So to summarize, we've got this hierarchy of different isolation levels which we inherited from System R in the mid 1970s, and nobody really understands what they mean, about you most people are using something somewhere along that hierarchy, a lot of people are using read committed because that's the default of about half the databases we saw on that list, but the interesting thing about read committed is it can actually be implemented without coordination and even something slightly stronger, introducing causality in that, can still be implemented without global coordination, so maybe there's some kind of way we can still do transactions across some kind of micro-services or stream processing architecture, which will obey casualty, which will obey read committed about but which doesn't introduce the whole overhead and problems of coordination. There's a lot of research going on in this area. I've got a totally oversized reference list for anyone who wants to read papers. This is page 1 of the references list, this is page 2 of the references list, page 3, and page 4, and so you can find them, all of the slides online. I've tweeted the link to the slides already. It's a big topic. And I've mostly described problems and not described many solutions, because many of the solutions are still unknown, but people are, working on them, if you fancy working on them, as well, please do contact me. As a final thing I wanted to say, I'm actually going to be giving away free copies of this book, O'Reilly have kindly printed off 25 copies even though it's not finished yet and I'll be giving those away during the lunch break. Otherwise there's a discount code if you want the e-book and I think I'm out of time I'm afraid. So if you want to ask me questions, I'll be around here and thank you very much for coming ...
   [applause]
