   
   Philip Wadler:  "Propositions as Types"
   
   (Live captioning by White Coat Captioning @whitecoatcapxg)
   >> Good morning. Are you ready to learn about the hilarious subject of computability theory?
   [applause]
   [cheers]
   >> An algorithm is a sequence of instructions followed by a computer. Now, you all think about computers as machines but for an awful long time what a computer was is a person, the person who executed the algorithm. Algorithms go back to Euclid's in Greene and Khwarizini, but a formal definition doesn't appear until the 20th century, when you have proposals by Alonzo Church, Kurt Godel and Alan Turing all appearing within a year of each other. It's like buses. You wait 2,000 years for definition of effective computability and then three come along at once. Why did this happen?
   >> So at the dawn of the 20th century, one of the foremost proponents of formal logic was David Hilbert and what he wanted to do was put all mathematicians out of business. What he wanted was an algorithm that given a statement in formal logic would determine if that statement was true or false.
   This was called the entscheidungsproblem. That sounds a lot better in German. It just means decision problem.
   What Hilbert was depending upon was the idea that logic is complete, meaning every provable statement is true, and every true is statement is provable. Sounds reasonable, right? Except of course in 1930, in Vienna, Kurt Godel published his proof of the incompleteness theorem and this meant that Hilbert was to use a technical term, screwed.
   So what Godel showed was that any power could result in this statement, this statement is not provable. What he did is used a clever technique called Godel. So don't worry about the details about how he did that although it is one of the world's first functional programs.
   But think about this statement, this statement is not provable. Oy! , right? As soon as you have this statement written down, you are in trouble?
   So what happens? If it's false, then it is provable, and you've proved something that's false. This is really bad news. You don't want to do that. So the alternative is that it's true. But now you must have a statement that is true but not provable. So that's not as bad, but it's still really annoying.
   Especially if you're Hilbert. Now, as long as people thought that there would be a solution to the problem, you didn't need a formal definition of algorithm, you would just write down the algorithm that was the solution and it would be just like Justice Potter's definition of pornography: "I know it when I see it."
   But if your goal is to show that the problem is undecidable then you need a formal definition for algorithm. So the race was on. The first solution was proposed by Alonzo Church in Princeton, he came up with this thing called lambda calculus in 1932 and by 1936, he had used it to show that yes, if an algorithm is what you can express in lambda calculus, then it is the case that problem is undecidable. There is the complete definition of lambda calculus. Much briefer than those of you that used languages like C or java. It's only got three constricts. Variables, function definition and function application, right and it's the world's coolest programming language because it was defined a decade before one had computers:
   For many years, me and my colleagues have worked with functional languages which are based on lambda calculus and for many years, people in industry have managed to pretty much ignore everything we do.
   Of course these days lambdas have become very trendy, you have lambdas in C++  , you have lambdas in Python and you have lambdas in Java. So there's duke, the icon for Java, looking very smug. Congratulations, Duke, you have finally caught up with where Alonzo Church was in the 1930s:
   So here we have back to Kurt Godel again. He came visiting in Princeton and he thought that Church's solution was thorough, his precise words were, thoroughly unsatisfactory.
   So Church went to Godel and he said, look, you come up with your own definition and I'll show that mine is as good as yours. This is actually written up about I Church's student Kleene, with attribution. And he went back to Godel expecting this would resolve the matter, and Godel said, oh, my definition is the same as your definition. Hm. My definition must be wrong, then.
   [laughter]
   The impasse was resolved by this man, Alan Turing at Cambridge who came up with what we call Turing machines. And he said that if your Turing machine was the same the entscheidengsproblem.
   It was not the mathematics. It was the philosophy. He gave and argument that anything that a computer could do could be done by a Turing machine, where again, computer means a person following a sequence of instructions.
   And Godel was finally convinced that all three equivalent definitions did. Philosophers often argue. Is mathematics invented or is it discovered? Three times, guys, three different independent definitions all turn out to be equivalent. That's powerful evidence that you've not invented something, you've discovered something. Right? It's not just sports fans who are impressed by a hat trick.
   [laughter]
   Kurt Godel was 28 when he undermined the work of David Hilbert. Alan Turing was 23, still an undergraduate when he undermined the work of Alonzo Church who was 33 and Kurt Godel was then an ancient 30. So to all you young people in the audience, please keep explaining to your elders when we are wrong. OK, so now you've got the background and woo can start talking about the brilliant idea of propositions as types. So here is Gerhardt Gentzen, he came up with a new way of writing down logic. Here it is. We're just going to focus, so this is called natural deduction. Gentzen in his Ph.D. thesis came up with natural deduction. He came up with sequence calculus which is the second most used formalism in logic today. He also came up with the upside down A to mean for all. So there's a little goal for all you Ph.D. students. So this is actually from Gentzen's paper and we're just going to look at a fraction of this, so the fractions having to do with implication and conjunction, so implication is that funny backwards C. It really is a backwards C. It's consequence backwards and ampersand which means and let's see, where's implication, and you can see it's exactly the same, except that Gentzen wrote his letters in German.
   [laughter]
   And then conjunction is up at the top there.
   So oops. Other way.
   So what do we have here? The first line, so the first rule there, the rules come in pairs. This is the important thing. From the left we have rules with I, meaning introduction rules and you have the connective implication or ampersand, below the line. The rules on the right are called elimination rules. And there you have a connective above the line. So you can see A implies B on the left there, and what does the one on the right say? The elimination rule? It says if you know A implies B and you know A. Those are the two hypotheses above the line and below the line we have the conclusion, what do you know, if you know A implies A and you know A, hey you have B. How do you create or introduce an implication? Well, those brackets around the A mean assume A. Don't prove A. Just assume A true. If you assume A is true and from assuming that is true, you can get a proof of B, then you know A implies B.
   And the second I'd say, well if you've got a proof of A and you've got a proof of B, you've proved A and B. And what can you do with that? You can do two different things. So the middle rule on the bottom says if you have proved A and B, you may conclude. A, and the other rule says if you've proved A and B, you may conclude B. OK. Are there any questions?
   [laughter]
   >> All right, I will zip on. But do ask a question if there's something that's confusing you. I'm sure it will be confusing other people, as well.
   >> So here's a little proof. It's a proof that if you know B and A, then you can conclude A and B. Now, this seems bloody obvious, right? Of course if I know B and A, I can conclude A and B. But it would be nice to actually have a formal proof. So here's the formal proof. It says OK, let's assume B and A and on the left we say if we've assumed B and A, we certainly know A and if. That's using the two different elimination rules. Now we've proved A and B so we know A and B and we can discharge our assumption, so with no assumptions now, we know that B and A implies B and A. Now, the key thing that Gentzen did that was amazing is he had the insight that the rules come in pairs and that the pairs cancel out.
   And he used this to prove what's called the subformula property, which says that if you have a proof, you can always normalize it by applying these rules. So that the only formulas that appear in it, so the formulas are our propositions here, are going to be the conclusion, the hypotheses and parts of those, what are called subformulas. No other formulas.
   So I'll give you an example of that in a minute, but let's look at the simplification rules. The top one says, OK I assumed A, I proved B so I know A implies B and I've also got a proof of A so I can conclude B. But there's a simpler way of doing this, right? We don't need to assume A, we were just given a proof of A so everywhere in that proof on the left that you have an assumption that A is true, just replace it with the proof you were given on the right that A is true and now we've P got another direct proof of B that doesn't use the formula A implies B. So we've got rid of a formula that doesn't appear in the conclusion or the hypotheses.
   Similarly, if from A and B, you can conclude A and B and then from that you can conclude B, well there's a as much simpler way of doing that, right? If you've got a proof of A, just use that, so proofs can be simplified. So let's do an example of that. Here's a round about way of proving B and. A. But we've just proved, right, that B and A implies A and B. So by modus ponens ... B and A implies B and A that don't appear. So we've got two undischarged hypotheses here and the B and A that has a little z on it, so we've got undischarged hypotheses, B and A and a conclusion, A and B, and a bunch of other stuff. Including stuff like B and A implies A and B that doesn't appear as a hypothesis or a conclusion. Can we get rid of it? Well, yes, because the second-to-last rule is an introduction, and our last rule is an elimination, so we can take those two places where we assumed B and A and replace them by the proof of B and A on the right. So it simplifies down to this and once we've taken that proof on the right and moved it on the top, we've now got two places here where and I is on top of and E. So we can simplify again and now we've got a direct proof. This is a much simpler proof of the same thing. Now, notice when you substitute something into a proof it may actually have more nodes in it but it will always be simpler in the sense that you've gotten rid of a subformula and you keep doing that until you've gotten rid of all the subformulas and that always works.
   Why did Gentzen care about this? Because it says OK, because just knowing that the proofs can be done in a direct way, that they weren't round-about, that's kind of cool but in particular you can do the following: One of the formulas you have is false and a logic is consistent if you cannot prove false. You really don't want to have any proof of false sitting around. If you did have a proof of false then the proof would look like false and only consisting of subformulas of that. Well, there are no subformulas of false, right? It's like what part of no don't you understand?
   [laughter]
   So it's very easy to look at the proof rules and say, ah, OK, there's no roof rule that ends in false, and therefore, you couldn't get it in any other way. So it gives a simple proof of consistency, among other things.
   Now, pretty much the exact same time that Gentzen was coming up with this new way of formulating logic, this is when Church was coming up with lambda calculus. Now he originally used lambda calculus as a kind of macro language for logic. It turned out that lambda calculus is really powerful. In fact it's so powerful it let you write down the equivalent of infinite formulas and with those you had an inconsistent logic, one that could prove everything. So that was kind of bad. But he did write in his original paper, it may have uses other than its use as a logic, in particular, it turned out to be good for defining algorithms.
   But also, he wanted a consistent system and to do that he used the same way of getting rid of pair docks that resiel used. So in 1940, church wrote down the simple termed lambda calculus. So here I have I have he a got terms in red and conclusions in blue and as well as functions I'm now dealing with pairs. You can also deal with things like record variance and pretty much every data type you'd name you can build up out of these ideas, but I'm just going to show you functions and pairs.
   So lambda XN is a term. It's a function. It's a function that given a value of Type A returns a value of Type B. So a function of A to B. And what does this mean? X must be a variable of Type A and it must appear in type N whose function is L: And L is an argument of Type A. Well, if you apply L to M, the result has Type B. And a pair is built from two terms of types A and B and it gives you an A-B pair and of course first and second extract if you've got an A-B pair, first would be an A and second would be a B.
   So here's an example of a program: Lambda Z return the pair second of Z first of Z. So this swaps elements of pair. And it's going a take a BA pair and return a AB pair. Right, so at this point I'd like you to all reach under your seats. You should find there there are some Rose-colored glasses. Please put on those Rose-colored glasses. You will then only see the blue bits and not the red bits and that should look kind of familiar.
   Right? So the blue bit of what Church did ask exactly what Gentzen did. Now, it actually took a long time to see this, because the proof I showed you that of Gentzen's result, Gentzen didn't prove it that way. He had to invent sequence calculus to prove it. It's kind of ironic. Gentzen needed a round-about proof to show the absence of round-about proofs.
   But then church came up with this and there are rules for evaluating lambda expressions, they're very simple. The first rule says if you've got lambda XN applied to M what does that mean? Well, take the actual parameter M and substitute it everywhere for the formal parameter X. And what does it mean given an MN pair, of course it just means return M. And again if you put on your rose-colored glasses you can see that correlation responds specifically to evaluation to simplification of proofs.
   The one thing that you need to know is this process terminates and that was actually first proved by Turing of all people. So remember the hard thing about the halting problem is that you can never solve it. But if it's in simply typed lambda calculus if you've typed it and you're not using the fixed program er it, yeah, that's the idea that Godel came up with. Then you're guaranteed that it terminates. So we have functional programming languages where we include the fixed point operator and you can write every general recursive program, that is, do everything a Turing machine can do, but we have other ones which are simply typed like this and examples of such languages are things like Ag da. So the halting problem is completely solved by this idea of types.
   And then here's an example of so here's the program that swaps two elements of the pair, applied to a particular pair, and of course we just substitute in the argument of why X. And we do that and we get that. So notice what those rules are telling us is that as we evaluate a program it stays well typed. So what have we seen in propositions in logic correspond to types in a programming language. Proofs in the logic correspond to terms, programs, in the programming language. And simplification of proofs corresponds to evaluation of programs. So it's not a shallow idea. It's a deep idea with a lot of structures. Propositions as types. Proofs as programs. Simplification of proofs as evaluation of programs.
   Ooh! You say, that's really cool!
   But hey, you know, it's just kind of an accident that it happens once like that, right? Well, no, it's not an accident. Oh, so this is sometimes culled the curry Howard isomorphism. So the idea showed up many times in the 20th century. You can find it in the work of brewer, hiking and the intuitionists and also it was noted by Haskell curry and the form that I'm showing you really was clarified by William Howard who then went on to say, well, wait a minute, if implication and correspond to all these things for what for all and there exists correspond to? And that came up with a new type that hadn't existed before called dependent types and these are the basis today of many proofs systems. It's sometimes called propositions as types. Anything that's really important, will of course, have lots of names.
   So there's houred's original paper, which was in fact published dedicated to Curry.
   Propositions as types, proofs as programs, and normalization of proofs as evaluation of programs. And as I said, well, fine, I showed you something that only works as intuitionistic logic. It's just kind of an accident, right? Well, no, it works for everything.
   So here's just a couple of ways in which it works. And the interesting thing to note here is for instance the logician hindly came up with type schemes and scientist millner. And notice now that it's now called the hindly millionner system but a computer scientist and a logician accidentally came up with the same thing.
   And it was covers discovered by the logician Girard. So curry Howard is a double barreled name that predicts there will be other double barreled names.
   [laughter]
   Every good idea will be discovered twice. Once by a logician, and once by a computer scientist.
   [laughter]
   And pretty much every functional language you can name has as its core the lambda calculus. That's pretty much the definition of a functional language.
   And interestingly, pretty much every proof assistant you can name, such as Coq or Agda or what have you so goes back to the math system of Dubroyne. So it's a very powerful idea. And of course just like any programming language, there are bits in all of these things that are completely arbitrary. But their core is not arbitrary. Their core is something that was written down once by a logician, and once by a computer scientist. That is, it was not invented, but discovered.
   Most of you use programming languages that are invented and you can tell. Can't you?
   [laughter]
   So this is my invitation to you to use programming languages that are discovered.
   [laughter]
   So Turing's important contribution was philosophy, so I'm going to indulge in a we bit of philosophy to conclude. Let's say that we tried to talk to aliens. We've actually done this, right? This is a plaque on the Voyager. And this diagram on the left is to try to show where is relative to various pulsars, there are marks on that that are actually in binary and of course the length of the line is the distance of sol from the various pulsars and then on the right there's a picture of some people. Now, if aliens were to look at this, right, the bit on the left, they would probably be able to work out, ah, that's the length of the line is distance and they could probably work out binary numbers and that's frequency and there's a little things there saying frequency of hydrogen. They could probably work that out. The thing hon the left, well, that would depend, right, on what the aliens are like. If Star Trek is correct, then the aliens would look at the bit on the left and say oh, they look just like us, except they don't have pubic hair.
   [laughter]
   But if aliens -- if Star Trek is not correct and aliens are really alien, they just might think the bit on the right is some scribbles that they cannot decipher. It all depends. So some things you can work out easily and other things you might might not be able to.
   Let's say that we try to communicate with aliens in a language so there's a movie called Independence Day and they destroy the aliens by giving them a computer virus and this is the computer virus and if you look at it closely it's written in C. It's actually a dialect of C that only has open brackets.
   [laughter]
   Of so this movie came out in the mid '90s, how do I know it's C and not Java? Well, that was the mid 90s, that was before Java spread throughout the known universe. So this seems kind of unlikely, right? Whether it's C or Java. It's unlikely you could program an alien computer using it, right? But what about lambda calculus? What lambda calculus? Lambda calculus isn't invented. It's discovered. If aliens know the fundamentals of logic, if they know the rule of modus ponens, then they must also know lambda calculus, so if we sent them a plaque a formula write in C, I think they might trouble deciphering it, but if we sent them a plaque in lambda calculus, I think that's something they might be able to work out. So lambda calculus would be the thing on the left that's easier to decipher than the squiggles on the right. So yes, we should call lambda calculus the universal programming language. Let's talk about that. It's become common about that to talk about multi-verses. This is from a play this only has one stage direction that says a horizontal rule in the script corresponds to a change of universe. So has entered our popular culture and it's also entered Ionesco and scientists say, why is the weak electron force the strength it is? If it was just a little bit stronger, electrons would repel and we wouldn't have matter and we wouldn't have life. Why is it the strength it is? Well, because we're in a universe where we can see it, so there must be matter and life so that's why the electron force has to be. So they reason using multiple universes.
   And multiple universe might have something like different electron force, different gravity. That's fairly easy to imagine. What about a universe without logic? Without modus ponens? I find that very difficult to imagine. So we really cannot call -- I'm sorry to say it but we can't call lambda calculus the universal programming language and the reason is because calling it universal is too limiting.
   [laughter]
   So in conclusion, right, what I'd like you to remember is that when you've got a tough job, you should think that this is a job for lambda calculus.
   [applause]
   [laughter]
   [applause]
   Thank you very much, and I will now take some questions.
   [laughter]
   Yes?
   >> Where can you get that shirt?
   >>
   >> I have tried many times to put this shirt online and every time I've done it, the online firm has said you can't do it, that's a copyright violation, so if you can tell me a way that I can do it, let me know and I will do it.
   >> Any other questions?
   >> Yeah?
   >> So you mentioned that industry has ignored lambda calculus for the past 50 years.
   >> E0, not just lambda calculus, they've ignored lots of things.
   >> Why do you think that's the case?
   >> Right, so the question is I claim that industry has ignored lambda calculus for many years and the request question is, why is this the case.
   >> If you look at good ideas like garbage collection, it came around with 50s and 60s in the mainstream community with the advent of java and even then many people fought against it. So it's like 25, 30 years before garbage collection got into widespread use. Even though we think of it as a very rapidly moving field, fundamental good ideas often time a very long time, 25 or 30 years, to be adopted. Which I will rant now for a minute. In the UK we're supposed to do impact studies to show the impact of our research and impact must be an idea published 20 years ago and used within the last 20 years, so I want to go OK, Java uses generics and I helped contribute to generics Java, so that's clearly an impact story, well, no, because the original work by robin millner was too old to count.
   >> If you look at the work of logic, all this stuff I showed you around 1935 and it was 1935, that Gentzen and Church came up independently with natural deduction and simply typed lambda calculus. It wasn't until late 1960s, 1969, that Howard wrote this down and it was just circulated as a xerox note. It wasn't published until 1980. So that's like 45 years.
   >> It takes a long time for good ideas to get out. And so it's worth having a long perspective if you have want to have the best and most interesting ideas. If you want things that are discovered and not invented.
   >> That was an excellent question, thank you. it let me rant.
   [laughter]
   Any other questions?
   >> Yes?
   >> Do you think that there are other systems that correspond to the same type --
   >> So the question was, in curry Howard we have a correspondence between logic and lambda calculus, are there any others? And of course right, we saw a few others. Right? So on the left here we have lots of different varieties of logic, and on the right we have lots of different features of programming languages. By the way there's one thing that's omitted from here which is the most important one, which is distribution and concurrency. And of course there are many. Different solutions to distribution and concurrency. What is the right one? Wouldn't it be great if there was some logic that corresponded to distribution and concurrency? That might give us a hint as an to idea that is discovered rather than invented and indeed linear logic on the left seems to correspond to things like session types on the right and of fact that's a major focus of my current research. So there's a possibility that we will be able to discover an approach to concurrency and distribution again that is discovered rather than invented but that's still ongoing work and of course once we're done it will then take another 25 years before people adopt it. So that wasn't quite your question because these are all different varieties of logic and different programming features, your question is, does it happen with other things? Well, yes, it happens with things like thermodynamics and information theory.
   >> So I guess my question is, is there -- do you think there's potential that there's a third column for --
   >> Oh, is there a third column or a fourth column? Yeah, so many people say that category theory is the third column. And that corresponds to both of these things. So yes, this actually goes deeper, there's a very nice paper by physicists talking about thesens, because they extend to things like quantum physics. So thank you for that question, that's an important point. But there might be other things waiting to be discovered.
   >> Do you think there will be a case where computer scientists will discover things before a logician?
   >> Oh, brilliant question. So this fellow was saying, look, the logicians always get there before the computer scientists. Will the computer scientists ever get there first? That's a very good question. I suss text the logicians always get there first because they've been at it longer. But for instance, linear logic was not discovered twice. Linear logic when Girard came up with it, was actually published in the journal theoretical computer science, because already this notion that there might be something there, was there at the time he discovered it. So in that sense, the two came together, and even, it turns out, John Reynolds had done something almost like linear logic just a few years before called syntactic control of interference in concurrent systems, so people refer to Reynolds coming up with this idea as just before Girard as gen Reynolds' revenge.
   >> Are there any probable is correspondences I suspect there are, but I don't know.
   >> What's it say about the impact of computer science on our understanding of knowledge in general?
   >>
   >> My favorite question. What dops this say about the impact of computer science on knowledge in general? Right? A lot of what we do day to day has arbitrary bits to it. This I've tried to explain to you is deep and not arbitrary. We're discovering things rather than inventing things. Clearly that has to have implications everywhere, and there's a growing movement that says, look, the ways of thought applied in computing, the things that you guys are all good at, give us new insights into how information is structured and structuring of information is not going to just be important for understanding how computers should be designed, it can also be important for understanding how the universe works. So this idea of using ideas of how information is structured, to examine many, many different fields, goes by the name of informatics, and some people work in departments of computer science. I actually work in a department -- a school of informatics. And I think informatics is a much better word to use. Right? Computer science, there are only two things wrong with it. The word computer and the word science.
   [laughter]
   Right? Because it's not about just the computer. It's about patterns of information. And you don't put science in your name if you're a real science.
   [laughter]
   >> Should I stop there?
   >> I think we'll stop there? Yes.
   >> Alex: It's a great place to stop.
   >> Thank you very much.
   >>
   [applause]
   >> I am' kindly there's an open slot, I'm talking tomorrow at 10:10 rather than at the unsession tonight. That's on domain-specific languages, so if any of you would like to come I'd be very pleased to see you there. Thanks very much.
   >> Alex: Lunch will be available outside. So please, everybody grab lunch, the Coder Girls' talk is in here at noon. If anybody wants to hear what they're up to. Thanks ... ...
   [break]
