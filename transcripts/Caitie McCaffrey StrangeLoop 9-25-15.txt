   Caitie McCaffrey:  "Building Scalable Stateful Services"
   
   (Live captioning by White Coat Captioning @whitecoatcapxg) 
   
   >> Hey, welcome, thanks for coming. I'm going to talk to you guys this afternoon about building scalable stateful services OK, so I'm Caitie McCaffrey. I currently work at Twitter and I'm the tech lead for observability there and prior to that I ... Building out services that powered those experiences. So what I'm going to talk to you guys today is some lessons learned in building stateful services and not databases and caches, but actually building stateful ability into our services. That's me on the internet. I will talk to you on the internet. So if you want to do that, hit me up. So we've been in this paradigm of programming stateless services and this has worked really well for about the last five to ten years and so your service is stateless and then as you get more clients or customers or users of your API you add another service and you scale it horizontally and this works pretty well and we use this databases that we talked to you about to store the canonical source of truth to be our history to be like the rock in our system that stored all of our states so that our services didn't have to think about it. And this works okay, and did for a while, but the problem is like our services do have state. The applications we're building have state and we've been replying on this abstraction of it's all like covered in this database and that gave us really strong consistency but we've started to hit these limits where one database doesn't cut it anymore. So we've gone into this world where we're sharing relational databases and we're giving many some of these and this abstraction of all the data living and being taken care of by the database is leaking now.  It's leaking into our services because what we're actually doing when we have state that we're using in stateless services is we have a client making a request in the data shipping paradigm and so I go to the database and it sends me some data back and then I answer the request and then I'm going to return the result, and then this data that I transferred over the network is just going to go away and the next time the client makes a similar request operating on a similar set of data it's going to do the ex a being same thing but it got load balanced to a different machine this time and so now we've pulled the same data across the network onto a different machine to answer the requests to the same user and it gets thrown away again and this keeps going on and on and we have these chatty clients that are operating over a session, over a duration or a period of time, like in games think in any of these applications where it's doing something for you, like ordering a car or being traing why you are fitness workout or you're communicating something about your state or location to the service and you you're mostly just talking about yourself because we all like to talk about ourselves. And pulling these all these different services load balancing between can be vety wasteful so I'm going to talk to you about how you might want to build stateful services and some of the about enfits you can get from them. This is not like a magic panacea that you want to use all the time. Horizontal scalability is something you still want to be able to do, but you can get some nice benefits we'll talk about the tradeoffs you'll have to make while building them I'll give you some real-world examples that are running today that have become stateful for a variety of different reasons and then I'll give you a cautionary road map of things you might want to watch out for than building stateless services so if you try to do this at home these are things that pop up that you may not think about.
   OK, so the big benefit I see in stateful services is you have data locality. So data locality is this idea that you ship the request to the machine that holds data on it and I want to do this for a couple of reasons. One I get low latency. I don't have to hit the databases every single time. I just have to hit it whenever I don't happen to have the state I need on that machine and so it's providing lower latency because we're doing less network calls. It's also reallocate for data intensive applications. So this is relying on the function shipping paradigms so this is the idea if a client makes a request or starts a session we still have to go to a database the first time to get the data and it moves into our service but then once that request has been handled we leave that data on the service and then the next time the client makes a very similar request, which is going to operate on that data, it just has to talk to the same machine that it already pulled the data onto and this way I don't have to go to my database, I'm not adding extra latency and even if my database is down, I can still handle this request, right? And then the same thing happens over and over again, as long as I need to not keep having to go back to the database, as long as I can talk to the machine that my data already lives on, this is ha nice world to live in.
   Another benefit of stateful services if you build them using sticky connections is you can get better consistency models.
   I think we're all familiar with the CAP theorem. You don't have to pick between two, because physics and in this world when we have different levels of consistency that we operate against, some are not available. And then if we want to have highly available forms of consistency you get write from read, monotonic read and monotonic write. If you if we have sticky connections then you can have these other consistency models that are strongly consistency. And so this is actually really nice.
   One of the first times I sort of came across this idea is a blog post entitled Essential Consistency Revisited, so he started talking about building these sticky connections and one of the points that he makes is in addition to consistency models is you're giving your clients a nicer framework to think about. If you just have your client talking to the same server because you have a sticky connection it's a little easier. OK, so those are many so of the benefits. Let's talk about how you may want to go about building he goes these so that you can take advantage of data locality so you can take advantage of some of these better consistency models and still be available.
   So the basic idea behind a sticky connection is it is exactly what it sounds like, we have a client who is going to make a request to a cluster of servers, they're always going to talk to the same server for their session for whatever the duration of what you're doing is going on. So the dumbest and easiest way is to open a persistent connection. It's super easy to implement so you'll always be sent to the same machine because you're always talking to that machine for the same session for the length of that session, however it does have some problems. Obviously once the connection is broken it the stickiness is broken and you're going to get rerouted. Load balancing when you do this, you're essentially making an assumption that all of these sticky connections last for the same time and you're going to cost the server they're talking to the same amount of load and that's not generally the case. You can easily overwhelm a single server by having too many consistent connections dogpiled that last for a long time or doing a ton of work and that machine is going to go down real hard if you do not implement back pressure so I highly recommend or would say that it's a requirement for building. This way the client then has to reconnect. You could also do some smart why are load balancing logic to say, like hey, connect to a machine that has the most resources free but you still need back pressure. That's a minimum requirement for consistent connections.
   If you want to do something a little smarter and this is more beneficial, you can implement some routing logic in your cluster, so this is the idea that a client can talk to any server in the cluster and then it will be routed to the server that has its data that's going to answer its requests where it's going to perform its computation and there's two sort of categories I think about is you have cluster membership, who's in my cluster and how do I determine what machines are actually available to be talked to and then work distribution. How am I going to distribute load across this cluster, right? So we're going to go through those because there's a variety of choices and they have different impacts and tradeoffs with them.
   So once again, the dumbest thing you can do, the easiest thing you can do which sometimes works. I'm always a fan with starting with the dumbest thing possible and see if it meets your needs. Is using static cluster membership. And this works for certain scenarios, although it does have problems. It's not super-fault tolerant, so if a machine goes down you essentially have to replace that machine. If you want to expand a cluster from an operations perspective this becomes incredibly painful because you have to take down the entire cluster and bring it back up in order to do the work distribution correctly.
   Animation has got ahead of me. So what's going on here is that this is easy to do but it's operationally painful and it's not a great choice for services that have to be highly available where you can't tolerate downtime or you have really strict SRAs. So the next better thing you can do is dynamic cluster membership an so this is just the idea that I can add and remove nodes on the fly from the cluster and because of failure or I want to add capacity they'll just get added to the cluster and they'll start immediately taking load and vice versa if one fails or you know, you can reduce capacity for whatever reason.
   So there's sort of two main big ways of doing dynamic cluster membership that I've seen work. There's gossip protocols that it's a protocol designed where the machines are going to chat just like you would with your friends and knowledge spreads throughout a social group and they're going to chat about who they can talk to and who's alive and who's dead and each machine on its own will figure out its world view of who is in a cluster, so it's just deciding based on the messages it's she have had. All of the machines in the cluster will stabilize as having the same world view. In the case where you have a bunch of network failures, capacity is being added or like you have a partition in your network, different machines in the cluster can actually have different world views of who is in the cluster. So this causes a bit -- this is like one of the tradeoffs that you need to make with dynamic cluster membership is if you want high availability, right, these machines don't have to coordinate to figure out who's in a cluster, they can independently make the decision based on the knowledge that they currently have, then you use a gossip protocol but your code and applications need to be able to tolerate this partition or this uncertainty where work may be getting routed to different nodes for brief periods of time during failure. Conversely if you need really strong consistently in cluster membership, everyone needs to have exactly the same world view you want to use a consensus system to do this, so you do this like in a typical way, you interact with a consensus system it controls the ownership of everyone who is in the cluster and they that changes everyone goes and updates their world view based on the system that's holding the truth of cluster membership. The problem with this is once again is that is system is not available the nodes can't reroute work. So sort of like Peter Bailis talked about we want to avoid this unless you need it.
   And then finally I want to talk, the second problem is work distribution, right, and there's a couple of different methods. I'm going to go through three. There's random placement, consistent hashing and distributed hash tables to figure out how to move work throughout your cluster.
   Random placement sounds super dumb but it can be effective in certain scenarios, so this is the idea that I get to write to anywhere in the cluster. So any machine that this has capacity just takes it and on read I go and I have to read from every single machine in the cluster to get the data back.
   So this isn't actually a sticky connection but it is a stateful service in the sense of this is a good way to build indexes memories and caches and I'll get to an example of how this is used in the real world because it sort of sounds stupid when you're just hearing the example. But it is useful when your queries is using a large amount of data that is going to be distributed over the entire cluster. I think one that is probably more familiar to people is consistent hashing. This came out of a paper in 1997 from the worldwide web about how to distribute workloads. The basic idea here is you have a deterministic placement of your requests and we've now mapped our nodes that are in the cluster to the same space as the requests that are coming in and so you've hashed your request based on whatever the session is or the user ID is depending on how you are want to partition your workload and the nodes get mapped to a cluster and the request comes in it also gets hashed onto a ring. This is really popular on a lot of databases use this Amazons, Twitter's Manhattan database uses this, I believe Cassandra uses this, I believe this is pretty basic database work distribution because it is deterministic placement, but the problem with deterministic placement I don't necessarily keep your data in memory all of the time like a database does, the problem with this is that you can have hotspots, right? You can run into the same problem where if your requests are not or even if they are evenly distributed because it's a hash, they actually have different weights in terms of the amount of work that they're gonna do, based on what the user wants to request. And so you could easily end up with a hot node or you could have a node that's slow in your cluster and it's going to get overwhelmed for a variety of reasons like maybe the discs are going bad or whatever. So this does not allow you to move work if a machine gets overloaded, so you have to allocate enough space in your cluster to allow enough headroom to determine that this is deterministic. So you're paying to have something sit around with enough capacity even in the normal case.
   So I'm finally going to talk about distributed hash table. So the idea is that I'm going to have a node look up a distributed hash table. So what happens is this holds a reference to the node that I'm going to go talk to that's in my cluster and this is nondeterministic placement, right? Because there's nothing that's forcing me to always go to Node A. I could easily remap in my client to go to node B or C if one gets too hot. I'll give you an example in the real world where this works very well.
   OK. So let's talk about stateful services in the real world. I'm going to give you an overview of three that I think are particularly interesting. And that sort of exhibit the range of many so of the stuff that we just talked about in terms of different ways you can configure and build your cluster out.
   So Scuba is from from Facebook: This runs a lot of Analytics and it has to be very, very fast and has to be very available because they're doing things on the fly, right? And I believe they use static cluster membership but the paper doesn't say that but from what I've inferred. I basically what happens is they do this random fan-out on write. So it's going to write on any machine that happens to have it and on read it's going to query every single machine on the cluster. And then all these results back in, they get composed by the machine that's like running the query and when the results are returned to the user a completeness metric is also returned. So what's cool about this is in order to get 100% of the data and you have all the rules, every machine in the cluster would have to be available but we live in the real world and that's never the case so what they did is they built this in from the start to say I'm going to tell the requester what percentage of data that I have been able to successfully retrieve data from, and then this allows them to decide if that's acceptable amount of uncertainty.
   And so I think that's actually really, really cool. There's a paper on SCUBA if want to read more. So they're not using sticky connections but they are using in memory for very, very fast data lookups.
   Another really interesting stateful service that has sort of come out in the last year is Uber built this thing called ring bob pop, it's a Node.js service. What the app is doing when you're ordering a car is talking to the service, it's going to send your location and throughout the duration of the ride it's going to and it has to process a bunch of payment and your driver stuff and if you that date with a constantly being persisted that's a ton of latency on the service so what they did is implemented routing logic in this clusters so they can have all of your requests for a session directed to a single machine so this do this. This is an AP cluster membership thing so it's not always guaranteed to be totally correct but they made the choice to always be available because they'd rather have you be able to order a car than to be like sorry, can't do this because Zookeeper is down. And they also used consistent hashing this is a really cool project. I highly recommend checking it out. It's open source on GitHub. And there have been a couple of extra talks and stuff around it if you want to know more.
 >>Finally I want to talk about Orleans. so Orleans came out of Microsoft research from the extreme computing group there and I actually had the pleasure of working with this group when we built halo 4 services. I've talked about Orleans before and I've talked about the aftermodel before and I'm just going to give you a quick overview. But really I want to talk about the routing logic in the system because it's very cool and it's sort of how Orleans does a lot of its magic. So what's happening is an actor model. Actors can communicate with one another by passing asynchronous messages to each other and when an actor receives an asynchronous message it can do one of four things or all of them. so essentially what I end up having in a cluster in any kind of actor model is a bunch of little state machines that are running and so you are inherently building stateful services when you are a he using the actor model and we use this very heavily at halo. We use them in little in memory right through caches. So how this actually works, though, is we just deploy a bunch of machines and Orleans run time takes care of the rest and so when we got a request it would go to any machine in the cluster and the cluster would look up where the actor lived in the cluster and route that message to it and there's likes hundreds of thousands of actors on any single machine in the cluster. So the way they do this is use a gossip protocol because we chose to be highly available. There's actually now that I think Orleans has been open is sourced a Zookeeper implementation but that's slower and so you are not going to get the performance benchmarks that are outlined in the paper.
   And it does this consistent hashing and a distributed hash that I'm going to walk through.
   So this is our Orleans clutter, we have 6 machines and there's a bunch of actors running on any machine. I'm going to get as request from a client that says hey I need to send a message to this actor and this ID. So it's going ply a consistent hash to that ID. So I'm going to go and then go to the machine that has the distributed hash entry for my actor, this will then tell me which machine my actor physically lives on and the query can be sent to the actor on the machine.
   That's going to be consistent throughout the lifetime of a actor and a cluster because it's not going to change. It's the same for every single node in the cluster so this is like actually very evenly distributed and balanced workload.
   However, what our actors are doing is not necessarily evenly distributed and balanced, because even different types of actors can run in Orleans clusters and so we wanted to make sure that if a machine becomes hot, Orleans can help rebalance the cluster for you.
   So what happens is say that machine got too hot or the session died and then the client comes in, the same client comes in, it will ask, like, hey, I need to talk to my actor again and so it will get routed to the same distributed hash table or the same node containing its piece of the distributed hash table and then the distributed hash table is now changed. The Orleans cluster has now up dated it to say hey, it's now on this machine and it does that for a couple of reasons. One like the machine you were talking to before failed. Or the actor got evicted from memory because no one's talking to to it any more or the machine got too hot. I'm going to use this capacity that's not being used on this other machine. This is one of the core reasons that. At so we're in the entire box because we were able to move work around in a nondeterministic fashion. So I think that was pretty cool and I think it's like, I would not recommend doing this for like a database, but it is really neat when you start thinking about pulling state into services.
   OK. So finally I'm going to talk about some things that are how you might run into trouble or like gotchas that you might not think about because you've been building stateless was is for a long time. So I think we all know or we should all know that unbounded queues are like essentially the devil in distributed systems and will kill you really, really fast because you're making implicit assumptions.
   In full state features, they will hurt you really, really fast and we don't really think about this in stateless services because you're assuming that you're processing a reasonable sized amount of data on any request and that is general garbage collected or evicted from memory at the end of that request. When you have stateful services and you're persisting a bunch of stuff over a span of requests, you need to put in explicit bound on here, because these could grow and you run out of memory on why machine or your garbage collector is really sad and decides it's going to -- and that node essentially looks dead.
   So this is something I've actually seen production machines go down because this assumption that we'd never run out of memory and our clients would only send us a reasonable amount of things in a session. Because clients are not your friends, they're not going to do what you want them to.
   You're also going to have to deal with memory management and you do have to deal with this in state less service to some extent. So basically because we're in a stateful world and we're persisting data for the lifetime or for the lifetime of the session which could be minutes, hours, whatever, days even, things are going to get persisted into the longest-lived generation of garbage collection. And so at this point, that's generally more expensive to collect. Especially if you have a bunch of references that are expanding generations of your garbage collection. So you have to think about it this more in stateful services or you could just say totally I'm not going to worry about it and go in and implement code. So Orleans runs dot net CLR. But we were able to work with our garbage collection and tune it and also realized we were persisting a lot of state that we never used so we sort of cleaned up our request. But you have to be careful about what you are persisting because this does have an associated cost with it.
   And then finally this sort of last gotcha is this idea of reloading state. Typically like in stateless services you're going to go to the database every single time and so you tune your latency to be acceptable based on having a round trip or you stick a cache in there to make it better. In stateful services there's a variety of different is cases, you can have the first connection of this session, so that's generally going to be your most expensive connection because there is no data anywhere why your service because it's probably been evicted if somebody hasn't talked to you forever or for a while. So you have to pull it in, you have to begin the startup requests, so you have to be very careful. You want it to look the best way possible. The best way to benchmark and test this is to use percentiles heavily. And so but and you will really only detect that your first connection startup time is really long if you use percentiles. Another sort of gotcha with connections is typically with stateless services I'm going to make a request and if the request times out I'm then just going to cancel the request on the database and not pull that state into memory. But with stateful services you know the client is going to come back and ask you for that data right away again and so you might as well just keep pulling it into memory even if the first request has failed and that client has timed out because they're going to ask for it again on a retry and it will get routed to the same machine and the data will be there and it will be fast. So typically in halo we have this issue where I think on game startup sometimes like the first connection would actually time out so we just kept pulling the data into the database and the next time it would success. We're talking hundreds of milliseconds of time. This actually wasn't really user noticeable since there was a pretty animation thing on the screen and so then by the time they came back the second time we just kept pulling the data that we needed especially if it was a player that played a lot or we had a ton of load because our azure table we were like stressing that connection. This worked really well for us.
   Another thing that's going to cause problems in terms of like reloading state is recovering from a crash. So if you have to like rehydrate an entire box after recovering from a crash that could be expensive if you're not lazily loading everything andisms too you can get away with lazily loading and sometimes you can't. Sometimes you have to take down an an entire box and bring up another box. So one interesting way to solve this that we sort of came across was or that I've come across is that Facebook published a paper called fast restarts at Facebook and this was actually designed for their SCUBA system. Because they had this in memory data that was persisted to hard disk so on a crash or in a deploy they would take down the process and they would spin up a new machine and they would have to read everything from disk and this took hours. It had a very slow reroll that took up to 12 hours. So this is going to slow down your development team a lot. Hopefully that happens fairly infrequently. Deploying code, though, happens really, really frequently or we'd like it to happen frequently because it allows us to iterate more. So they made this key observation that you can uncouple memory lifetime. So when they wanted to deploy new code and they knew it was a safe shutdown and memory wasn't corrupted because I told the process to shut down, it didn't crash, they would stop taking requests, they would actually copy all of the data into shared memory and shut down the old process and bring up the new process that would then copy the data from a shared memory and then they would start taking requests. This took minutes so they got their cluster resource time down to two hours. So this allows them to deploy much more frequently than they were able to before. So that's a really easy trick that is really cool and we're actually looking at implementing it at one of the services that I'm responsible for at Twitter. So in conclusion, I hope I've sort of painted a picture of why you may want to start bringing state into your actual services and not just relying on your databases, there are some really nice properties, there is a ton of thought that needs to be put into cluster membership and work distribution and I don't have one right answer for you though I tend to lean towards the available said side so I'd go with gossip protocols. There are a bunch of these real world systems out there that are running and so like it's been proven at scale that you can do this and so it's not necessarily super scary although it is new ground and finally be cautious, this is new territory if you haven't done stateful services before, go through what is different, what that is changed, what assumptions are you making and make sure you make them explicit and then finally I have a sort of like, this is new space and people aren't doing this, so should I even bother reading papers? Yes, yes, you should, because most of what I have talked about has come from database literature and show these problems are already soft for solved for you and you don't -- you were the nice thing about implementing sincerely ofs is you get to cherry pick what you care about. You can pick the pieces of the paper that you like and that problem is probably solved for you so I highly recommend reading papers, do not reinvent your own protocols. This is new territory. People have been working on this since like the 60s and 70s.
   Finally I want to say thank you to some people who helped me out with this talk. Thank you, guys so much for all of your help, and that's all I have for you. So I think we have time for like one question.
   >>
   [applause]
   >> No one wants to be that one question?
   >> Cool, well I'm going to be around if you guys want to come and chat with me afterwards I'm more than happy to talk about this. Thank you so much.
   [applause]
